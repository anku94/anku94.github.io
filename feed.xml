<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://anku94.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://anku94.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-13T18:15:09+00:00</updated><id>https://anku94.github.io/feed.xml</id><title type="html">Ankush Jain</title><subtitle>Personal website and blog of Ankush Jain, PhD Student at CMU PDL. Topics include systems, storage, networks, HPC etc. </subtitle><entry><title type="html">On reasoning, autonomy, intelligence etc.</title><link href="https://anku94.github.io/blog/2024/reasoning/" rel="alternate" type="text/html" title="On reasoning, autonomy, intelligence etc."/><published>2024-04-21T21:31:58+00:00</published><updated>2024-04-21T21:31:58+00:00</updated><id>https://anku94.github.io/blog/2024/reasoning</id><content type="html" xml:base="https://anku94.github.io/blog/2024/reasoning/"><![CDATA[<p>So the topic for this post is a bit out of my wheelhouse. The primary purpose of this post is to flush some thoughts that are brewing in my head so that I can get back to my day job. Everything else is secondary. I should warn anyone reading this that I have no particular background in these topics, nor have I done a decent job of keeping up to date on related work. This post will have no references — it may be completely off-track, or reasonable but outdated, or exist in any other defined epistemic state. <em>Caveat Emptor.</em></p> <p>Over this post, I will explore five topics — intelligence, autonomy, reasoning, planning, and consciousness. This has been obviously prompted by the hottest thing to happen this decade (LLMs). Let me structure this post as a series of theses and rationales.</p> <h2 id="llms-as-currently-designed-can-not-be-reasoning">LLMs, As Currently Designed, Can Not Be Reasoning</h2> <p>This idea emerged in my head in the early days of GPT-3. Its prowess was shocking, and everyone started reassessing everything (<em>autogaslighting</em>).</p> <p>This intuition is simple — LLMs generate text at a constant token rate, and an intelligent agent simply can not do that. For a simple query, an intelligent agent is able to respond instantly, while for a complex query, it needs to go and <em>think</em> and <em>plan</em> a response. There can not be any upper bound to the amount of time this takes.</p> <p>LLMs seem to encode some constant amount of capability. It is perfectly okay for a new (yet not fully optimized architecture that is on the right track) to take more time than necessary for a simple query, but an architecture that can not take an infinite time can not be reasoning.</p> <p>More specifically, an inference agent that is thinking and planning will have some <code class="language-plaintext highlighter-rouge">while</code>-loop like structure in some shape or form.</p> <h2 id="intelligence-is-poorly-defined">Intelligence Is Poorly Defined</h2> <p>I am sure there is material by scientist-philosophers exploring the definition of intelligence. I have not read any of it. The thesis is that it is am ambiguous phrase encapsulating all the flaws of natural language, and is best avoided. The other four words, in my head, lend to much cleaner definitions.</p> <ol> <li><strong>Reasoning</strong>. The ability to model a situation as statements in some form of logic (say propositional logic), and prove them to be correct as per the rules of that logic.</li> <li><strong>Autonomy</strong>. The ability to operate independently in pursuit of some objective.</li> <li><strong>Planning</strong>. The ability to decompose a complex problem into a series of subproblems. This is very much like A* search — it is recursive, and may require backtracking. It is essentially a solver, and benefits from some “intuition”, which is a set of heuristics that produce a partial solution quickly and speed up the solver. It also requires an ability (intrinsic or external) to know if the objective has been met.</li> <li><strong>Consciousness</strong>. It is simply a state of heightened autonomy, where an agent is able to choose its objective function, and change it at any point. <h2 id="consciousness-emerges-from-autonomy-and-reasoning">Consciousness Emerges from Autonomy and Reasoning</h2> </li> </ol> <p>In the previous subsection, I was trying to go for definitions that allow the respective traits to exist independently, albeit complementarily. I do not think I fully succeeded.</p> <p>An ant, I think, has autonomy and planning. I find it hard to separate the two — an autonomous object that can not plan would just not survive (survival! life! an objective function! natural selection!).</p> <p>It was very hard to define planning as something independent of reasoning. I think that that’s possible, though. An ant does not need to be able to reason that it has eaten and that it can temporarily stop hunting for food. It either reaches that state or “nature” lets it die. Over time, only ants that can recognize that state survive. Even then, they can only recognize specific states, and can not reason in general terms. <em>Reasoning is not necessary for autonomy.</em></p> <p>I think it also holds in the other direction. <em>Autonomy is not necessary for reasoning</em>. A solver that matches a set of statements to a type of logic does not have to be autonomous.</p> <p>Planning, I think, is not strictly necessary for reasoning, but it is key to making it an interesting trait. A solver that can only evaluate given statements for compatibility with the axioms of a logic would be a boring solver. A solver that can translate a <em>situation</em> into valid logical statements, and reconcile them with some past state of the world in its head is far more interesting. Does this imply/require autonomy? I think it does not.</p> <p>Finally, the combination of autonomy and reasoning is powerful. Such an agent is autonomously able to recognize the achievement of a certain objective (without an incentive from nature). This leads to an ability to choose between objectives, which, I argue, is a sufficient condition for the emergence of what I call consciousness.</p> <p>Computationally, an autonomous reasoning conscious agent has a randomly accessible set of objectives. It context switches to some objective based on some meta-heuristic, and runs an A* search on that objective. If it fails, it knows that it has failed, and keeps backtracking. If it succeeds, that objective is marked completed. A <em>smarter</em> agent is either quick in succeeding, or succeeds more often than others. But both smart and less-smart agents have the same fundamental architecture, and an ability to recognize that an objective is not met.</p> <h2 id="an-ai-agent-does-not-necessarily-need-to-be-conscious">An AI Agent Does Not Necessarily Need To Be Conscious</h2> <p>If reasoning and planning can be accomplished without autonomy, that is sufficient to get to a really powerful level of assisting agents. If such agents are not autonomous, they are not conscious, and this allows us to circumvent a whole host of ethical concerns.</p> <p>This would be somewhat akin to taking a snapshot of a 25-year old’s head, and using that snapshot to run queries. Since the snapshot is not autonomous, it does not accumulate more knowledge, but is able to serve an infinite amount of queries being a “non-alive” “non-conscious” program.</p> <h2 id="incorporating-logic-into-training">Incorporating Logic Into Training</h2> <p>How do you train your models in a way that produces reasoning and planning?</p> <p>One way may be that the training process is seeded with a preferred logic and its axioms. Each training iteration does something akin to producing an A* plan on a given goal, validating the plan with respect to the logic, and using the loss to make changes to various components if the logic is violated.</p> <p>The components could be:</p> <ol> <li>The A* “intuition”</li> <li>The natural language translation “intuition” (translate natural language into logical statements and vice-versa)</li> <li>A world model (as a series of statements/metastatements in the logical format)</li> </ol> <p>Each “intuition” IMO, is a proxy for a neural net. The entire apparatus is a series of neural nets and an architecture that are co-trained, until they start converging to some reasonably low loss.</p> <h2 id="can-logic-emerge-naturally">Can Logic Emerge Naturally?</h2> <p>Is it possible that some deep learning architecture automatically learns something that resembles this structure? After all, we humans developed different logics from scratch. Maybe there are other sets of axioms that are “better” and these models automatically stumble upon them?</p> <p>I think hacking this loop manually is necessary, because we want agents that can reason and plan, but not be autonomous. Autonomy requires us to think about all sorts of complicated things — are these things alive, should they have rights, do they like us, are they aligned the way we want them to? Best to circumvent these if possible.</p>]]></content><author><name></name></author><category term="ml"/><category term="#ml"/><category term="#llm"/><category term="#reasoning"/><summary type="html"><![CDATA[Or what would it take to get to agents with these properties.]]></summary></entry><entry><title type="html">An Exercise In Debugging When The Kernel Is Implicated</title><link href="https://anku94.github.io/blog/2024/quis-custodiet/" rel="alternate" type="text/html" title="An Exercise In Debugging When The Kernel Is Implicated"/><published>2024-04-15T20:22:10+00:00</published><updated>2024-04-15T20:22:10+00:00</updated><id>https://anku94.github.io/blog/2024/quis-custodiet</id><content type="html" xml:base="https://anku94.github.io/blog/2024/quis-custodiet/"><![CDATA[<p>Situation: an MPI application (10s of processes) seems to freeze after completion. Once it freezes, I can’t attach to any of its processes via <code class="language-plaintext highlighter-rouge">gdb -p</code> (that also freezes). I can’t <code class="language-plaintext highlighter-rouge">SIGTERM</code> or even <code class="language-plaintext highlighter-rouge">SIGKILL</code> the processes. <code class="language-plaintext highlighter-rouge">ps aux</code> (my standard set of ps flags) shows the processes in either an <code class="language-plaintext highlighter-rouge">Is</code> state, or a <code class="language-plaintext highlighter-rouge">Ds</code> state.</p> <p>Apparently, <code class="language-plaintext highlighter-rouge">D</code> processes are waiting for I/O in a non-interruptible state. <code class="language-plaintext highlighter-rouge">I</code> processes are idle, and are in an interruptible state. If I tried to poke around in <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;</code> too much, even procfs would freeze.</p> <h2 id="wchan">wchan</h2> <p>The first useful thing I learned was the <code class="language-plaintext highlighter-rouge">wchan</code> column in <code class="language-plaintext highlighter-rouge">ps</code>. <code class="language-plaintext highlighter-rouge">wchan</code> means wait channel - it tells you what stream (or whatever the definition of a channel is) is a process waiting on.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ -eo ppid,pid,user,stat,pcpu,comm,wchan -p 21061 | grep stoch
  21028   21060 ankushj  Zs    0.3 stoch &lt;defunct&gt; -
  21028   21061 ankushj  Is    0.4 stochastic_subg ptlrpc_set_wait
  21028   21062 ankushj  Zs    0.2 stoch &lt;defunct&gt; -
  21028   21068 ankushj  Zs    0.2 stoch &lt;defunct&gt; -
  21028   21069 ankushj  Ds    0.4 stochastic_subg rwsem_down_write_slowpath
  21028   21070 ankushj  Ds    0.4 stochastic_subg rwsem_down_write_slowpath
  21028   21071 ankushj  Zs    0.4 stoch &lt;defunct&gt; -
  21028   21072 ankushj  Zs    0.4 stoch &lt;defunct&gt; -
  ...
</code></pre></div></div> <p>Bingo - we have some function names to blame. Apparently we can get more context around these without needing to attach a debugger, which we can not.</p> <p>We can <code class="language-plaintext highlighter-rouge">cat</code> <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;/stack</code> to get a stack trace from a process without a debugger. Thankfully this works.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo cat /proc/21061/stack

[&lt;0&gt;] ptlrpc_set_wait+0x5e8/0x730 [ptlrpc]
[&lt;0&gt;] ptlrpc_queue_wait+0x88/0x230 [ptlrpc]
[&lt;0&gt;] ldlm_cli_enqueue+0x436/0x990 [ptlrpc]
[&lt;0&gt;] mdc_enqueue_base+0x2f2/0x1c90 [mdc]
[&lt;0&gt;] mdc_intent_lock+0x212/0x530 [mdc]
[&lt;0&gt;] lmv_intent_lock+0x385/0x16b0 [lmv]
[&lt;0&gt;] ll_lookup_it+0x7ad/0x20e0 [lustre]
[&lt;0&gt;] ll_atomic_open+0x198/0xff0 [lustre]
[&lt;0&gt;] lookup_open+0x364/0x6e0
[&lt;0&gt;] do_last+0x2cb/0x900
[&lt;0&gt;] path_openat+0x8d/0x290
[&lt;0&gt;] do_filp_open+0x91/0x100
[&lt;0&gt;] do_sys_open+0x17e/0x290
[&lt;0&gt;] __x64_sys_openat+0x20/0x30
[&lt;0&gt;] do_syscall_64+0x57/0x190
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo cat /proc/21065/stack

[&lt;0&gt;] rwsem_down_write_slowpath+0x244/0x4d0
[&lt;0&gt;] do_last+0x2b1/0x900
[&lt;0&gt;] path_openat+0x8d/0x290
[&lt;0&gt;] do_filp_open+0x91/0x100
[&lt;0&gt;] do_sys_open+0x17e/0x290
[&lt;0&gt;] __x64_sys_openat+0x20/0x30
[&lt;0&gt;] do_syscall_64+0x57/0x190
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
</code></pre></div></div> <p>This is beautiful! Instead of gaslighting ourselves and slipping into an existential crisis, we can point our fingers elsewhere.</p> <h2 id="conclusion">Conclusion</h2> <p>Both these stack traces indicate getting stuck on a File I/O call. The first trace is more specific - it tells us that it is a Lustre I/O. Lustre is a parallel filesystem.</p> <p>Turns out that one of my Lustre OSTs was dead. That would explain this behavior - the Lustre module sits in the kernel, and is blocked trying to reach that OST. Maybe there are good reasons a timeout there can not be handled gracefully, maybe there are not. But a kernel module freezing is tough - a whole bunch of debugging options go for a toss. Thankfully some still work!</p>]]></content><author><name></name></author><category term="debugging"/><category term="#systems"/><category term="#gdb"/><summary type="html"><![CDATA[The problem here was being caused by a kernel module misbehaving because a remote service was misbehaving.]]></summary></entry><entry><title type="html">Replication Part 1 - Primary-Backup vs Chain Replication</title><link href="https://anku94.github.io/blog/2024/replication-part-1/" rel="alternate" type="text/html" title="Replication Part 1 - Primary-Backup vs Chain Replication"/><published>2024-04-11T17:35:30+00:00</published><updated>2024-04-11T17:35:30+00:00</updated><id>https://anku94.github.io/blog/2024/replication-part-1</id><content type="html" xml:base="https://anku94.github.io/blog/2024/replication-part-1/"><![CDATA[<p>The purpose with this series of posts, to the extent it is followed through, is to arrive at different replication schemes by applying diffs to some well-understood schemes. This exercise should help us understand the tradeoffs, the design space, and reason about whether a point in the design space is pareto-optimal, that is, can its pros be realized by a diff over some other design point with less cons.</p> <p>This is somewhat facetious, but IMO there are two classes of people in the distributed systems world — Leslie Lamport, who stared into the soul of replication and figured out something fundamental a couple of decades ago and has since been trying to tell us all that everything in replication is a specialization of that [principle][1,2,3], and everyone else.</p> <p>Part 1 will cover 3 basic designs. For now, we will assume single-key operations and key-value semantics, and see how far this simple model gets us.</p> <p>Let me also try to assign relative perf numbers for these, where 100 is the max perf for a single-node design (writes and reads are assumed to be symmetric cost-wise, so a 100 perf for writes and reads just means that any combination of the two will have a perf of 100. Perf = latency = throughput as we can assume one request in flight at a time, WLOG?). So a perf of 500 implies 5X the read throughput of a single node.</p> <h5 id="1-a-single-worker">1. A Single Worker</h5> <p>(Worker could be a core, or a node, or a datacenter even. We relax consistency for performance all the way down.)</p> <p>Design: a single worker, receives all requests, applies them sequentially, writes see reads, works great, no issues.</p> <p>Consistency: Strict Failure: No tolerance Read Perf: 100 Write Perf: 100</p> <h5 id="2-primary-backup">2. Primary-Backup</h5> <p>Two workers. All requests are handled by the primary. All writes are synchronously copied to backup. Backup kicks in on failure.</p> <p>Consistency: Strict Failure: Tolerates 1 Read Perf: 100 Write Perf: 50 (assuming synchronous copying = as costly as query)</p> <h5 id="2a-primary-backup-with-async-replication">2a. Primary-Backup with Async Replication</h5> <p>Primary can ACK to a request before replication to backup is completed. It is a meaningless design point because async replication has no tolerance for failures. The delta between the two can be lost.</p> <h5 id="2b-primary-backup-with-backup-serving-reads">2b. Primary-Backup with Backup Serving Reads</h5> <p>The backup is right there, and it is idle. Can we use it?</p> <p>We won’t use it for writes, because concurrent writes to the same key can cause conflicts and we do not yet know how to resolve them. Also we have not evolved and coordination mechanism yet. But we could use it for reads… right?</p> <p>But it should respond to reads without requiring coordination with the primary, otherwise we have not really gained anything in terms of performance. But that has consistency implications — what if the primary is concurrently updating the value we returned?</p> <p>This works under two cases:</p> <ol> <li>A looser consistency model, Sequential Consistency, is acceptable, OR</li> <li>The Primary and Backup can establish a coordination mechanism that is more efficient than what the clients can do (sharding keys is one example of such a mechanism)</li> </ol> <p>For case 2b1.</p> <p>Consistency: Sequential Failure: Tolerates 1 Read Perf: 200 Write Perf: 50</p> <h5 id="3-chain-replication">3. Chain Replication</h5> <p>Chain Replication is just a generalization of 2b. Instead of having one replica, you can have N replicas. In fact, for 1:N primary-read serving backup systems, I think these two are logically equivalent:</p> <ol> <li>The primary chaining writes sequentially through replicas</li> <li>The primary broadcasting writes to all replicas</li> </ol> <p>First option increases latency by N, and the second option requires the primary to do $N\times$ more communication.</p> <p>I think the two only have perf implications, but are logically equivalent to each other and to 2b.</p> <h4 id="references">References</h4>]]></content><author><name></name></author><category term="systems"/><summary type="html"><![CDATA[Simplifying scheme soup with successive steps]]></summary></entry><entry><title type="html">Journey Through A CMake Dependency Problem</title><link href="https://anku94.github.io/blog/2024/journey-thru-a-cmake/" rel="alternate" type="text/html" title="Journey Through A CMake Dependency Problem"/><published>2024-02-22T20:17:06+00:00</published><updated>2024-02-22T20:17:06+00:00</updated><id>https://anku94.github.io/blog/2024/journey-thru-a-cmake</id><content type="html" xml:base="https://anku94.github.io/blog/2024/journey-thru-a-cmake/"><![CDATA[<p>Situation: I have a CMake project that needs <code class="language-plaintext highlighter-rouge">libfabric</code>, which needs <code class="language-plaintext highlighter-rouge">rdma-core</code>.</p> <p>Both of these are non-CMake Projects, but they generate <code class="language-plaintext highlighter-rouge">pkg-config</code> files in <code class="language-plaintext highlighter-rouge">/prefix/lib/pkg-config</code>.</p> <p><strong>Q. How to locate these libraries as dependencies in my CMake project?</strong></p> <p>A. Use CMake’s <code class="language-plaintext highlighter-rouge">PkgConfig</code> package.</p> <pre><code class="language-CMake">find_package(PkgConfig)
pkg_check_modules(LIBFABRIC REQUIRED IMPORTED_TARGET GLOBAL libfabric&gt;=1.14)

if (LIBFABRIC_FOUND)
  message(STATUS "LIBFABRIC_INCLUDE_DIRS: ${LIBFABRIC_INCLUDE_DIRS}")
  message(STATUS "LIBFABRIC_LIBRARY_DIRS: ${LIBFABRIC_LIBRARY_DIRS}")
else()
  message(FATAL_ERROR "LIBFABRIC not found.")
endif()

target_link_libraries(exec PRIVATE PkgConfig::LIBFABRIC)
# Side note: I don't know what GLOBAL does
# I thought it'd get the target out of the pkg-config namespace. Apparently not.
</code></pre> <p><strong>Q. Ok, code was built, but it can not locate libraries at the custom path.</strong></p> <p>A. You could use <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code>, but that’s honestly annoying. With <code class="language-plaintext highlighter-rouge">RPATH/RUNPATH</code>, your binary knows where to find the packages it was linked to.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ldd exec
        linux-vdso.so.1 (0x00007ffe733f7000)
        libfabric.so.1 =&gt; /opt/fi_bench/libfabric/fab-prefix/lib/libfabric.so.1 (0x00007ff4d8924000)
        libstdc++.so.6 =&gt; /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff4d873b000)
        libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff4d8720000)
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff4d852e000)
        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff4d8526000)
        librdmacm.so.1 =&gt; not found
        libibverbs.so.1 =&gt; not found
        libnuma.so.1 =&gt; /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007ff4d8519000)
        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff4d850e000)
        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff4d84eb000)
        libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff4d839a000)
        /lib64/ld-linux-x86-64.so.2 (0x00007ff4d92ea000)
</code></pre></div></div> <p>Why can it not find <code class="language-plaintext highlighter-rouge">librdmacm.so.1</code> or <code class="language-plaintext highlighter-rouge">libibverbs.so.1</code>? We run <code class="language-plaintext highlighter-rouge">make VERBOSE=1</code> to intercept the compile command.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/c++ <span class="nt">-Wl</span>,-rpath,/opt/fi_bench/libfabric/rdma-core-50.0/build/lib CMakeFiles/server.dir/server_main.cpp.o CMakeFiles/server.dir/common.cc.o CMakeFiles/server.dir/fabric.cpp.o CMakeFiles/server.dir/endpoint.cpp.o CMakeFiles/server.dir/benchmark.cpp.o <span class="nt">-o</span> server  <span class="nt">-Wl</span>,-rpath,/opt/fi_bench/libfabric/fab-prefix/lib:/opt/fi_bench/libfabric/rdma-core-50.0/build/lib /opt/fi_bench/libfabric/fab-prefix/lib/libfabric.so /opt/fi_bench/libfabric/rdma-core-50.0/build/lib/libibverbs.so
</code></pre></div></div> <p>We see that the build process is supplying the relevant rpaths. Why are they not being resolved?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ readelf -d executable | head

Dynamic section at offset 0x5c60 contains 31 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [libfabric.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x000000000000001d (RUNPATH)            Library runpath: [/opt/fi_bench/libfabric/rdma-core-50.0/build/lib:/opt/fi_bench/libfabric/fab-prefix/lib:/opt/fi_bench/libfabric/rdma-core-50.0/build/lib]
</code></pre></div></div> <p>So apparently <code class="language-plaintext highlighter-rouge">server</code> does not have <code class="language-plaintext highlighter-rouge">libibverbs.so</code> as a dependency! So it doesn’t care about its rpath. It must be coming from <code class="language-plaintext highlighter-rouge">libfabric.so</code> then.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ldd libfabric.so.1
        librdmacm.so.1 =&gt; not found
        libibverbs.so.1 =&gt; not found
        libnuma.so.1 =&gt; /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007f534861b000)
        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f5348610000)
        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f53485ed000)
        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f53485e5000)
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f53483f3000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f5348fee000)
</code></pre></div></div> <p>Why does <code class="language-plaintext highlighter-rouge">libfabric</code> not preserve rpaths?</p> <p>At this point I don’t care. I just hacked the rpaths into LDFLAGS.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ verbs_PREFIX=$RDMA_PATH/build verbs_LIBDIR=$RDMA_PATH/build/lib LDFLAGS="-Wl,-rpath,$RDMA_PATH/build/lib" ./configure --prefix=/opt/fi_bench/libfabric/fab-prefix
</code></pre></div></div> <p>Afterwards,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ldd libfabric.so
        linux-vdso.so.1 (0x00007ffecb1d0000)
        /users/ankushj/repos/stderred/build/libstderred.so (0x00007f9e73c24000)
        librdmacm.so.1 =&gt; /opt/fi_bench/libfabric/rdma-core-50.0/build/lib/librdmacm.so.1 (0x00007f9e73c07000)
        libibverbs.so.1 =&gt; /opt/fi_bench/libfabric/rdma-core-50.0/build/lib/libibverbs.so.1 (0x00007f9e73be5000)
        libnuma.so.1 =&gt; /lib/x86_64-linux-gnu/libnuma.so.1 (0x00007f9e73bd0000)
        librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f9e73bc5000)
        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f9e73ba2000)
        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f9e73b9a000)
        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9e739a8000)
        libnl-3.so.200 =&gt; /lib/x86_64-linux-gnu/libnl-3.so.200 (0x00007f9e73985000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f9e745e2000)
        libnl-route-3.so.200 =&gt; /lib/x86_64-linux-gnu/libnl-route-3.so.200 (0x00007f9e7390b000)
</code></pre></div></div> <p>You can love systems, rarely do they love you back.</p>]]></content><author><name></name></author><category term="systems"/><category term="systems,"/><category term="cmake"/><summary type="html"><![CDATA[Or how to craft artisan ELF files with exquisitely detailed metadata]]></summary></entry><entry><title type="html">Indian Union Budget - Expenditure Analysis</title><link href="https://anku94.github.io/blog/2023/govt-exp/" rel="alternate" type="text/html" title="Indian Union Budget - Expenditure Analysis"/><published>2023-11-11T04:35:31+00:00</published><updated>2023-11-11T04:35:31+00:00</updated><id>https://anku94.github.io/blog/2023/govt-exp</id><content type="html" xml:base="https://anku94.github.io/blog/2023/govt-exp/"><![CDATA[<p>Check <a href="https://anku94.github.io/dash/sankey">this</a> out! (Code <a href="https://github.com/anku94/dataviz">here</a>.)</p> <p>Government budgets are opaque. They utterly, absolutely, completely dwarf smaller social sector organizations trying to make anything better. They are also poorly understood.</p> <p>This is an attempt to demystify them - mostly for myself, maybe for others. This is also a WIP, and will (may?) get better over time. Understanding these datasets better helps us understand entrenched interest groups that have captured a disproportionate share of these sums (a typical pattern in democracies).</p> <p>Some quick notes on the process:</p> <ol> <li> <p>Indian govt. is organized as ministries, which are internally organized as departments. Some departments are directly managed by the PMO (I think?) and do not have a parent ministry as such. Each ministry creates multiple demands for grant, one for each department.</p> </li> <li> <p>I parsed a giant Excel sheet containing 100+ of these demands for grants, and organized them into a tree-like structure. This process was painful, and the python code that does this is a reflection of that. There are a couple of places where things get rounded off due to heuristics breaking down, but they should not cause massive issues.</p> </li> <li> <p>The data visualization in in a decent JS stack (NextJS, React, Typescript, Plotly.js …). Feel free to contribute!</p> </li> </ol>]]></content><author><name></name></author><category term="data"/><category term="js,"/><category term="goi"/><summary type="html"><![CDATA[Expenditure analysis and visualization of the Indian Govt. Union Budget for FY2023-24.]]></summary></entry><entry><title type="html">The Varied Flavors of Infiniband</title><link href="https://anku94.github.io/blog/2023/infiniband-flavors/" rel="alternate" type="text/html" title="The Varied Flavors of Infiniband"/><published>2023-09-11T19:05:42+00:00</published><updated>2023-09-11T19:05:42+00:00</updated><id>https://anku94.github.io/blog/2023/infiniband-flavors</id><content type="html" xml:base="https://anku94.github.io/blog/2023/infiniband-flavors/"><![CDATA[<p>This is an attempt to track and understand all the different “flavors” of Infiniband-like interfaces that are around, active, or relevant.</p> <p>Thinking of things like PSM, PSM2, OFI, Verbs, UCX etc. I doubt this particular post will have anything insightful — this is more of a placeholder for more insightful things in the future.</p> <h2 id="pre-historic-times-1995-2002">Pre-historic Times (1995-2002)</h2> <p>Most HPC clusters had proprietary networks, with their own <a href="https://agullo-teach.gitlabpages.inria.fr/school/school2019/slides/mpi.pdf">MPI providers</a>. Some weird-sounding names: MPICH-MX, MPICH-Elanlab.</p> <h2 id="ancient-times-1995-2002">Ancient Times (1995-2002)</h2> <p>InfiniBand: long awaited HPC network standard. Comes with OFED open-source networks stack, the <code class="language-plaintext highlighter-rouge">verbs</code> API. MPI implementations started using Verbs.</p> <h2 id="pre-modern-2005-2010-and-modern-2010-now-times">Pre-Modern (2005-2010) and Modern (2010-Now) Times</h2> <h3 id="performance-scaled-messaging">Performance-Scaled Messaging</h3> <p>Needs supposedly start outstripping the capabilities of the standards. PSM (Performance-Scaled Messaging) is introduced.</p> <h4 id="psm-owner-history">PSM Owner History</h4> <p>There used to be this company called QLogic. They developed network hardware, including “Infiniband”, in the pre-modern times.</p> <p>Their IB was compatible with the Verbs API, but was developed truly for this interface called PSM. They acquired PathScale (a compiler vendor, among other things maybe?) in 2006, and developed <em>InfiniPath</em>, also marketed as <em>TrueScale</em> — a network interface that was Infiniband-compatible but supported PSM for better performance.</p> <p>They were later sold to Intel. Their tech became the basis of a second-gen interconnect called <em>Omni-Path</em>. Omni-Path used an evolution of psm called psm2, which was not backward-compatible. The psm/Qlogic stuff was 40 Gbps, psm2/Omni-Path was 100 Gbps. As per <a href="https://www.nextplatform.com/2023/08/24/cornelis-unveils-ambitious-omni-path-interconnect-roadmap/">this</a>, Omni-Path also incorporated features from Gemini and Aries interconnects bought from Cray in 2012.</p> <p>Something happened at Intel around 2019 — they canceled the 200 Gbps variant of Omni-Path. In 2020, a company called Cornelis Networks was spun out to carry Omni-Path forward.</p> <p><a href="https://www.nextplatform.com/2021/07/09/a-third-dialect-of-infiniband-in-the-works-again/">Cornelis</a> still exists, and are continuing the development of Omni-Path-based products.</p> <h4 id="psm-technical-details">PSM Technical Details</h4> <p>This is more of questions and references.</p> <blockquote> <p>“The software infrastructure of InfiniBand, based on verbs, is really based on the original goals of InfiniBand, which was to replace PCI-X and Fibre Channel and maybe Ethernet,” Murphy tells The Next Platform. “Verbs were not structured at all for high performance computing. PathScale created Performance Scale Messaging, or PSM, which was totally independent of InfiniBand verbs and was a parallel transport layer specific focused on HPC. In the enterprise, when I am talking to 40 or 50 disk drives or 40 or 50 queue pairs, I can put that on my adapter’s cache and it works great. But in HPC, when I have a node with a hundred cores and a thousand nodes, this becomes a giant scalability problem we just cannot manage in the adapter’s cache. PSM could do this better, but even this was invented two decades ago and the world has continued to evolve.</p> </blockquote> <p>It seems one of the considerations for PSM was the NIC SRAM overhead of creating Verbs Queue Pairs. NIC SRAM is finite, and for all-to-all type communication patterns, the SRAM can not fit all the contexts. This <a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-kalia.pdf">paper</a> also talks about prefering RDMA UDs over RCs. With RDMA UD, one queue pair can be used for multiple destinations, and the total number of QPs can be controlled. We don’t want CPU cores contending over UDs, so one QP/core is still desirable, but it scales with the number of cores, instead of the number of destinations, which is a lot more tractable for large clusters.</p> <p>One of the references also talks about dynamic congestion detection and rerouting borrowed from Aries and incorporated into Omni-Path. As per <a href="https://www.youtube.com/watch?v=E0uSl_gyZnI">this</a>: QLogic thought that the best way to provide scaling was to avoid hardware offload entirely, and have everything be done by building on efficient Host CPU-based primitives. Datacenter hardware trying to support TCP, as we know in hindsight, went in the opposite direction, and now we’re talking about DPUs and SmartNICs.</p> <h3 id="mellanox-and-nvidia">Mellanox and Nvidia</h3> <p>Something about Mellanox hacking Verbs, with “Accelerated Verbs”, MXM, UCX, RoCE, GPUDirect …</p> <h3 id="hpe-and-slingshot-and-ultraethernet">HPE and Slingshot and UltraEthernet</h3> <p>Similar stuff?</p> <h3 id="then-there-is-cxl">Then there is CXL</h3> <p>Phew.</p>]]></content><author><name></name></author><category term="systems"/><category term="networks"/><summary type="html"><![CDATA[Some description]]></summary></entry><entry><title type="html">Playing With Perf Probes - II</title><link href="https://anku94.github.io/blog/2023/more-perf/" rel="alternate" type="text/html" title="Playing With Perf Probes - II"/><published>2023-09-11T17:37:12+00:00</published><updated>2023-09-11T17:37:12+00:00</updated><id>https://anku94.github.io/blog/2023/more-perf</id><content type="html" xml:base="https://anku94.github.io/blog/2023/more-perf/"><![CDATA[<h3 id="problem-need-sudo-to-probe">Problem: Need Sudo To Probe</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perf probe <span class="nt">-x</span> ./code <span class="nt">--add</span><span class="o">=</span><span class="s1">'add_with_sleep:0 sleep_us'</span>
</code></pre></div></div> <p>This will fail without <code class="language-plaintext highlighter-rouge">sudo</code> unless you’re root. A solid <a href="https://www.kdab.com/wp-content/uploads/stories/Linux_perf_for_Qt_developers.pdf">setup</a> that fixes most perf permissions issues:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">sudo </span>mount <span class="nt">-o</span> remount,mode<span class="o">=</span>755 /sys/kernel/debug
<span class="nb">sudo </span>mount <span class="nt">-o</span> remount,mode<span class="o">=</span>755 /sys/kernel/debug/tracing
<span class="nb">echo </span>0 | <span class="nb">sudo tee</span> /proc/sys/kernel/kptr_restrict
<span class="nb">echo</span> <span class="nt">-1</span> | <span class="nb">sudo tee</span> /proc/sys/kernel/perf_event_paranoid
<span class="nb">sudo chown </span>root:tracing /sys/kernel/debug/tracing/uprobe_events
<span class="nb">sudo chmod </span>g+rw /sys/kernel/debug/tracing/uprobe_events

</code></pre></div></div> <h3 id="probing-internal-library-functions">Probing Internal Library Functions</h3> <p>You’re going through the source code of your library, and there’s a specific internal function you want to profile. But it’s not listed in <code class="language-plaintext highlighter-rouge">perf probe -x exec -F</code>. It probably isn’t listed in <code class="language-plaintext highlighter-rouge">nm -a exec</code> either. If your functions are using this:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="n">__inline__</span> <span class="n">FN</span> <span class="nf">__attribute__</span><span class="p">((</span><span class="n">always_inline</span><span class="p">));</span>
</code></pre></div></div> <p>There’s no way they’re making it to the compiled output. This must be deleted. With <code class="language-plaintext highlighter-rouge">__inline__</code> and <code class="language-plaintext highlighter-rouge">inline</code>, things vary. There are two things I needed to get gcc to preserve my symbols up to the final <code class="language-plaintext highlighter-rouge">.so</code> object:</p> <ol> <li>Remove <code class="language-plaintext highlighter-rouge">always_inline</code> from code</li> <li>Add <code class="language-plaintext highlighter-rouge">-fvisibility=default</code> to my compiler flags</li> <li>Add <code class="language-plaintext highlighter-rouge">-fno-inline</code> to counter the anti-debugging forces channeled by <code class="language-plaintext highlighter-rouge">-O3</code></li> </ol> <p>More references:</p> <ul> <li><a href="https://minervadb.xyz/wp-content/uploads/2020/12/Dynamic-Tracing-for-Finding-and-Solving-MySQL-Performance-Problems-on-Linux-MinervaDB-Database-Platforms-Virtual-Conference-2020.pdf">MinervaDB</a></li> <li><a href="https://www.spinics.net/lists/linux-perf-users/msg02465.html">MailingList</a></li> </ul>]]></content><author><name></name></author><category term="systems"/><category term="perf"/><summary type="html"><![CDATA[Using perf probes for dynamic instrumentation]]></summary></entry><entry><title type="html">Playing With Perf Probes</title><link href="https://anku94.github.io/blog/2023/perf-probes/" rel="alternate" type="text/html" title="Playing With Perf Probes"/><published>2023-09-08T19:56:00+00:00</published><updated>2023-09-08T19:56:00+00:00</updated><id>https://anku94.github.io/blog/2023/perf-probes</id><content type="html" xml:base="https://anku94.github.io/blog/2023/perf-probes/"><![CDATA[<p>This post demonstrates the power of perf probes, using the following example C code. Let’s say we’d like to verify that usleep actually sleeps for the claimed amount of time. This is the output we can get.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sleep Arg: 0 us, Sleep Time: 92 us
Sleep Arg: 100 us, Sleep Time: 167 us
Sleep Arg: 200 us, Sleep Time: 262 us
Sleep Arg: 300 us, Sleep Time: 370 us
Sleep Arg: 400 us, Sleep Time: 468 us
Sleep Arg: 500 us, Sleep Time: 584 us
Sleep Arg: 600 us, Sleep Time: 684 us
Sleep Arg: 700 us, Sleep Time: 784 us
Sleep Arg: 800 us, Sleep Time: 885 us
</code></pre></div></div> <p>Code for this exercise.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">add_with_sleep</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="n">sleep_us</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">usleep</span><span class="p">(</span><span class="n">sleep_us</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>

<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">add_with_sleep</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">100</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"I: %d, Ret: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">ret</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>Series of commands. You can run them line by line.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>

setup<span class="o">()</span> <span class="o">{</span>
  <span class="nb">echo</span> <span class="nt">-1</span> | <span class="nb">sudo tee</span> /proc/sys/kernel/perf_event_paranoid
<span class="o">}</span>

run<span class="o">()</span> <span class="o">{</span>
  gcc <span class="nt">-g</span> <span class="nt">-o</span> code code.c
  ./code

  <span class="nb">alias </span><span class="nv">perf</span><span class="o">=</span>~/.local/bin/perf

  <span class="c"># -g enables call graph</span>
  perf record <span class="nt">-g</span> ./code

  <span class="c"># DWARF enables better stack unwinding</span>
  perf record <span class="nt">-g</span> <span class="nt">--call-graph</span><span class="o">=</span>dwarf ./code
  perf report <span class="nt">--stdio</span>

  <span class="c"># Show probe-able functions in binary</span>
  perf probe <span class="nt">-x</span> ./code <span class="nt">-F</span>

  <span class="c"># Show probe-able function lines in binary</span>
  perf probe <span class="nt">-x</span> ./code <span class="nt">-L</span> add_with_sleep

  <span class="c"># Show probe-able function lines in binary</span>
  perf probe <span class="nt">-x</span> ./code <span class="nt">-L</span> add_with_sleep:2

  <span class="c"># Show available variables at some point</span>
  perf probe <span class="nt">-x</span> ./code <span class="nt">-V</span> add_with_sleep:2

  <span class="c"># Add probes for function entry and exit. At entry, we also capture the arg.</span>
  <span class="c"># If the arg is say a string, we can specify varname:string to deref the str ptr.</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-x</span> ./code <span class="nt">--add</span><span class="o">=</span><span class="s1">'add_with_sleep:0 sleep_us'</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-x</span> ./code <span class="nt">--add</span><span class="o">=</span><span class="s1">'add_with_sleep%return'</span>

  <span class="nb">sudo </span>perf record <span class="nt">-e</span> probe_code:add_with_sleep <span class="nt">-e</span> probe_code:add_with_sleep__return ./code

  <span class="c"># This will generate a python script called perf-script.py</span>
  <span class="c"># Modify its entry and exit functions to print deltas between</span>
  <span class="c"># Entry and Exit timesteps</span>
  perf script <span class="nt">-g</span> python

  <span class="nb">sudo chown</span> <span class="si">$(</span><span class="nb">whoami</span><span class="si">)</span> perf.data
  perf script <span class="nt">-s</span> perf-script.py | less

  <span class="c">## Cleanup ##</span>

  <span class="c"># Should show two events</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-l</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-d</span> <span class="s1">'add_with_sleep'</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-d</span> <span class="s1">'add_with_sleep%return'</span>
  <span class="c"># Should show zero events</span>
  <span class="nb">sudo </span>perf probe <span class="nt">-l</span>


<span class="o">}</span>

run
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">perf script -g python</code> will generate a file that looks something like this. Incorporate these changes into the script.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">start_arg</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">probe_code__add_with_sleep</span><span class="p">(</span><span class="n">event_name</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">common_cpu</span><span class="p">,</span> <span class="p">...):</span>
    <span class="k">global</span> <span class="n">start</span><span class="p">,</span> <span class="n">start_arg</span>
    <span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="n">common_secs</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">)</span> <span class="o">+</span> <span class="n">common_nsecs</span>
    <span class="n">start_arg</span> <span class="o">=</span> <span class="n">sleep_us</span>

<span class="k">def</span> <span class="nf">probe_code__add_with_sleep__return</span><span class="p">(</span><span class="n">event_name</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">common_cpu</span> <span class="p">...):</span>
    <span class="k">global</span> <span class="n">start</span><span class="p">,</span> <span class="n">start_arg</span>
    <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="n">common_secs</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">)</span> <span class="o">+</span> <span class="n">common_nsecs</span>
    <span class="n">time_usec</span> <span class="o">=</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e3</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sleep Arg: {:.0f} us, Sleep Time: {:.0f} us</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">start_arg</span><span class="p">,</span> <span class="n">time_usec</span><span class="p">))</span>
</code></pre></div></div> <h3 id="credits">CREDITS</h3> <ol> <li><a href="https://bristot.me/using-perf-probe-to-measure-execution-time-of-user-space-code-on-linux/">https://bristot.me/using-perf-probe-to-measure-execution-time-of-user-space-code-on-linux/</a></li> <li><a href="http://notes.secretsauce.net/notes/2019/12/16_c-probes-with-perf.html">http://notes.secretsauce.net/notes/2019/12/16_c-probes-with-perf.html</a></li> <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/creating-custom-circular-buffers-to-collect-specific-data-with-perf_monitoring-and-managing-system-status-and-performance">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/creating-custom-circular-buffers-to-collect-specific-data-with-perf_monitoring-and-managing-system-status-and-performance</a></li> <li><a href="http://blog.vmsplice.net/2011/03/how-to-use-perf-probe.html">http://blog.vmsplice.net/2011/03/how-to-use-perf-probe.html</a></li> <li><a href="http://oliveryang.net/2016/07/linux-perf-tools-tips/">http://oliveryang.net/2016/07/linux-perf-tools-tips/</a></li> </ol>]]></content><author><name></name></author><category term="systems"/><category term="perf"/><summary type="html"><![CDATA[Using perf probes for dynamic instrumentation]]></summary></entry></feed>