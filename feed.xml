<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ankushja.in/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ankushja.in/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-14T17:45:40+00:00</updated><id>https://ankushja.in/feed.xml</id><title type="html">Ankush Jain</title><subtitle>Personal website and blog of Ankush Jain, PhD Student at CMU PDL. Topics include systems, storage, networks, HPC etc. </subtitle><entry><title type="html">Notes on Deepseek 3FS Filesystem</title><link href="https://ankushja.in/blog/2025/notes-on-deepseek-3fs/" rel="alternate" type="text/html" title="Notes on Deepseek 3FS Filesystem"/><published>2025-03-02T14:19:21+00:00</published><updated>2025-03-02T14:19:21+00:00</updated><id>https://ankushja.in/blog/2025/notes-on-deepseek-3fs</id><content type="html" xml:base="https://ankushja.in/blog/2025/notes-on-deepseek-3fs/"><![CDATA[<p>This is just me making some quick notes on the 3FS Parallel Filesystem from Deepseek<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. Caveat that while I have opinions on all systems, filesystems is strictly not my day job and I have to go back and think about a bunch of things and it doesn’t quite work, and this post will likely have a bunch of mistakes.</p> <h1 id="services-in-3fs">Services in 3FS</h1> <ul> <li>Cluster manager: highly available, manages membership, config etc (uses zookeeper?)</li> <li>Metadata service: uses FoundationDB</li> <li>Storage service (chunk store?)</li> <li>Client (two implementations: FUSE-based, and a more performant one.)</li> </ul> <h1 id="metadata-in-3fs">Metadata in 3FS</h1> <h3 id="the-ongoing-metadata-debate">The Ongoing Metadata Debate</h3> <p>Whether we need object stores or filesystem semantics has been a topic of conversation recently<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. My own views so far:</p> <ul> <li>I buy the argument that data accesses during a training run do not need a namespace. Maybe the namespace can be <em>batch-updated</em> later on, if it gets in the way and does not provide the required performance.</li> <li>I feel that the usability argument still applies. At some point, you need to think about what does all the data mean, what to keep around, what to prune. There are a million reasons – legal, regulatory, financial, just basic IT maintenance that require metadata. Long-term, it is easy to think about things as a group or a hierarchy.</li> </ul> <p>I think performant fileystem namespaces is doable. It may get needlessly hard if you tie yourself to a spec or a standard. That is because this is in flux, and we do not yet have unanimous agreement on what the standard should look like.</p> <h3 id="the-3fs-approach">The 3FS Approach</h3> <p>They use a key-value model for their metadata, built on top of FoundationDB. Each key represents an inode.</p> <p>FoundationDB, as an aside, is a very interesting database. It is like a distributed RocksDB with support for strict serializability in transactions. This is interesting to me — normally you think of KV stores as single-node storage backends for DBMSes to use. Transactions are implemented at a SQL level. FDB implements transactions at the KV level, but is supposed to be a lower-level database you layer on top of. (Aside: I wonder what the end-to-end principle says about the correct layering). It is like a persistent distributed software transactional memory, that tries to be lock-free, with optimistic and multi-version concurrency control.</p> <ul> <li>File inodes are <code class="language-plaintext highlighter-rouge">INOD&lt;inode_id&gt;</code>, mapped to ownership, permissions, times., and chunk information etc. for files. Interestingly, they encode <code class="language-plaintext highlighter-rouge">inode_id</code> in little-endian to spread inodes across FDB nodes. Directory inodes begin with <code class="language-plaintext highlighter-rouge">DENT</code>, have bidirectional links to parents to ensure no loops.needs to be and should be super-fast.</li> <li><code class="language-plaintext highlighter-rouge">fstat</code>, <code class="language-plaintext highlighter-rouge">lookup</code>, <code class="language-plaintext highlighter-rouge">listdir</code> etc. invoke read-only txns. <code class="language-plaintext highlighter-rouge">crate</code>, <code class="language-plaintext highlighter-rouge">link</code> etc. invoke read-write txns.</li> </ul> <h3 id="state-in-3fs">State in 3FS</h3> <p>3FS metadata stores are stateless. Any operation is a self-contained transaction.</p> <p>The metadata service does store file descriptors for files opened in write mode, but not in read mode. This helps with bootstrapping training jobs. This IMO is super important!! Any parallel job, when bootstrapping, needs to retrieve a large number of files in read-only mode. This should be super-fast, but is not so with Lustre. I vaguely recall someone mentioning that they had to hack on a cache on top of Lustre to push <code class="language-plaintext highlighter-rouge">.so</code> libraries to MPI ranks, and thinking that this feels like a fundamental flaw in what is supposed to be a parallel filesystem.</p> <p>Question: how do you handle a write request for a file that is currently open in read-only mode? Ideally you need to track the readers and revoke their lease, but that requires maintaining state at the metadata layer? I think that’s done by tracking versions?</p> <h2 id="chunk-store">Chunk Store</h2> <p>This is relatively straightforward. It is per-node, and comes with a RocksDB instance for chunk metadata. Chunk metadata is cached in-memory. They have 11 chunk sizes from 64 KiB to 64 MiB. I don’t think this is a big deal but metadata shouldn’t be too much, even with 64KiB chunks, it is a little surprising that they went for this number of chunk sizes. But eh. Updates are CoW – old chunks remain valid until update completes.</p> <h2 id="chain-replication">Chain Replication</h2> <p>They use CRAQ (Chained Replication with Apportioned Queries) for replication. This is just a fancy way of saying that any replica can respond to a read request, with the catch being that you may get a stale version. A node may have a committed version of a block and a pending version. If so, it lets the requester know the existence of both versions, and the decision on whether to tolerate staleness or try again is left to the requester. Note that this is different from the case where a chain replica does not even know of the existence of an update – I don’t know if stale reads are tolerated in that case (or maybe version numbers are used to read a consistent snapshot).</p> <h2 id="zero-copy-in-fuse">Zero-copy in FUSE!?</h2> <p>They use FUSE for clients, so as to not deal with debugging kernel panics. They define an interface called <code class="language-plaintext highlighter-rouge">USRBIO</code>, for zero-copy interaction between the userspace and the FUSE layer. <code class="language-plaintext highlighter-rouge">USRBIO</code> is inspired by <code class="language-plaintext highlighter-rouge">io-uring</code> (or the Verbs API for that matter). The FUSE process manages Verbs-registered memory and submission/completion rings, and does the dispatch etc. These zero-copy ops are only used for the data path, and metadata ops still go through regular FUSE APIs.</p> <h2 id="misc">Misc</h2> <ul> <li>Codebase is mostly C++ with some Rust.</li> <li>They use <code class="language-plaintext highlighter-rouge">P</code><sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> to verify their protocols. <code class="language-plaintext highlighter-rouge">P</code> is apparently a formal verification language for event-driven distributed services. Very cool!</li> <li>They seem to use <code class="language-plaintext highlighter-rouge">flatbuffers</code> for serdes and their own RDMA/RPC wrappers on top of the <code class="language-plaintext highlighter-rouge">verbs</code> interface. There’s a lot of code there and I just briefly skimmed through it.</li> <li>They heavily use <code class="language-plaintext highlighter-rouge">folly</code>, Facebook’s assorted library of C++ abstractions (including coroutines).</li> </ul> <h1 id="epilogue">Epilogue</h1> <p>Not sure what to make of this. Writing a parallel filesystem is a massive undertaking. While metadata bottlenecks is a well-known problem in parallel filesystems, offloading it entirely to a DB like FoundationDB is an interesting choice. I have been trying to revisit how Colossus/Tectonic manage their metadata, and it is Bigtable and ZippyDB respectively. Ceph is pretty much the only open-source filesystem to support multiple metadata servers (Lustre is unwieldy in multiple ways). 3FS is optimized for RDMA fabrics over the data path, and makes subtle choices (like cheap metadata reads) to optimize for a large parallel job bootstrapping. I remember reading somewhere that it is designed for small random accesses, and I don’t really see why that is the case. I will end with the caveat that there are a lot of things to keep straight in distributed/parallel filesystem discussions, I don’t really work on these, and this post probably has a bunch of mistakes.</p> <p>(Oh their design notes do say that they enable random access to training samples, I don’t see how they optimize for random accesses, maybe they mean that their system will transfer partial chunks and not do aggressive prefetching etc?)</p> <h1 id="references">References</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://github.com/deepseek-ai/3FS <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>https://blog.glennklockwood.com/2025/02/llm-training-without-parallel-file.html <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>https://github.com/p-org/P <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="systems"/><category term="filesystems"/><category term="storage"/><summary type="html"><![CDATA[Notes on the Fire-Flyer Filesystem by Deepseek]]></summary></entry><entry><title type="html">On The Universality of Query Languages</title><link href="https://ankushja.in/blog/2025/universality-query-languages/" rel="alternate" type="text/html" title="On The Universality of Query Languages"/><published>2025-02-06T15:08:30+00:00</published><updated>2025-02-06T15:08:30+00:00</updated><id>https://ankushja.in/blog/2025/universality-query-languages</id><content type="html" xml:base="https://ankushja.in/blog/2025/universality-query-languages/"><![CDATA[<p>This is an initial quick draft of this line of thought, written 16 hours before a paper deadline I have things to do about (that has nothing to do with query languages). It will be rushed and half-baked, and surreptitiously refined later.</p> <p>Goal for this post: let us try justifying SQL. Why it exists, why is it in a specific form etc.</p> <h2 id="sql-vs-relational-algebra">SQL vs Relational Algebra</h2> <p>I think SQL is made-up and non-essential. It is syntactic sugar over relational algebra. Other declarative interfaces may be more convenient abstractions — I personally prefer dataframes and chaining of expressions.</p> <h2 id="on-the-universality-of-relational-algebra">On the Universality of Relational Algebra?</h2> <p>Okay so why does relational algebra exist in the form it does?</p> <p>As a lower bound on this argument, a query interface could just be say… C. You write a blob, machine runs blob, you get output.</p> <p>We do declarative interfaces to make life easier given certain domain-specific constraints. In the relational world, it is:</p> <ol> <li>Leveraging the relational/tabular data model.</li> <li>Leveraging distributed cluster resources.</li> <li>Leveraging fine-grained scheduling, concurrency etc.</li> <li>Getting easy access to indexes etc.</li> <li>Enabling reduced data movement by exposing primitives that capture data flow</li> <li>Enabling query optimization by exposing primitives that make it easier to reason about ordering and statistics</li> <li>Enable reasoning about partitioning and shuffling using key constraints and joins</li> </ol> <h2 id="do-relational-operators-do-these-things">Do Relational Operators Do These Things?</h2> <p>To a large extent, yes. That is why relational algebra endures. SQL just pretends to be inseparable from that and most people do not ask too many questions.</p> <p>You always need escape hatches. That is why UDFs exist. But beyond a point, they hinder the query planning infra’s ability to reason about what the blob does.</p> <h2 id="who-deserves-to-be-in-the-club">Who Deserves To Be In The Club?</h2> <p>What operators should be in this club?</p> <ul> <li>Can we justify the existence of all existing operators?</li> <li>Are there new ones that should be there but are not?</li> <li>Under what models do we need radically different solutions?</li> </ul> <p>Relational algebra checks out as it is essentially set theory. Given a table, you can slice it horizontally or vertically. Indexing helps with horizontal slicing. Vertical slicing is harder in row-based stores but column-based stores have solutions. Selection and projection map nicely to dataflow, indexing etc. Division seems to be the only relational operator not directly used in query plans.</p> <h2 id="where-am-i-getting-at">Where Am I Getting At?</h2> <p>There exist data models (such as multidimensional array-based scientific data) that are not relational. Graphs are one example. Nothing here is really new.</p> <p>The question I am trying to answer is: what is the set of ideas that lead you to the perfect operators for these domains? Do they exist? Are they in conflict with relational algebra? Or can they be members of the extended family?</p> <p><em>Note to self: read referenced papers[^1,2,3]</em></p> <h2 id="references">References</h2>]]></content><author><name></name></author><category term="philosoraptormode"/><category term="sql"/><summary type="html"><![CDATA[Why is SQL what it is? What motivates a different query language?]]></summary></entry><entry><title type="html">Are All Networks Just Tradeoffs?</title><link href="https://ankushja.in/blog/2024/network-tradeoffs/" rel="alternate" type="text/html" title="Are All Networks Just Tradeoffs?"/><published>2024-12-07T00:58:30+00:00</published><updated>2024-12-07T00:58:30+00:00</updated><id>https://ankushja.in/blog/2024/network-tradeoffs</id><content type="html" xml:base="https://ankushja.in/blog/2024/network-tradeoffs/"><![CDATA[<p><a href="https://ankushja.in/blog/2024/credits-flow-congestion/">Part 1 here</a>.</p> <p>This post will explore the following thesis:</p> <ol> <li>All networks and variants are tradeoffs.</li> <li>If there are tradeoffs, there is no permanent winning. You are always tuning to chase <em>the window</em>.</li> </ol> <p>Disclaimer: this could be the most obvious statement in the world. Or maybe deep within the symbolic mines of queuing and scheduling, someone has said something that disproves all of it. Or maybe that that statement is obvious but the arguments below are flawed. I have no idea — this blog does not come with any guarantees for rigor or scrutiny.</p> <h2 id="context-thinking-about-cbfc-vs-pfc">Context: Thinking About CBFC vs PFC</h2> <p>The thing in the back of my mind recently has been — why does everyone complain about PFC and RoCEv2, and why do they not complain about Infiniband and CBFC (Credit-based Flow Control). Why could RoCEv2 not adopt CBFC? What do you gain and what do you lose? Do you still need higher-level congestion control with CBFC?</p> <h2 id="cbfc-is-not-a-magic-pill">CBFC Is Not A Magic Pill</h2> <p>An upper bound to CBFC goodness is much easier to establish than its precise extent.</p> <p>CBFC is not a magic pill. It can still lead to credit loops, head-of-line blocking, buffer overflows, packet loss emanating from it, and so on. It is also amenable to tuning: routing, topologies, credit management schemes etc. to reduce the likelihood of bad things happening.</p> <p>CBFC absolutely provides no theoretical guarantee (I’m not talking about a bounds-type argument: under X flows and Y buffer sizes you prove a Z\% upper bound on packet loss). Also not everyone means the same thing when they talk about CBFC — the Infiniband spec on this is not very prescriptive — my understanding is that it describes packet formats, and some basic mechanics, but still leaves a lot of room for vendors to do better.</p> <h2 id="guarantees-are-expensive">Guarantees Are Expensive</h2> <p>You could provide a lot of guarantees by implementing a network as an all-to-all network of separate links. That is essentially what a crossbar is. But we want to optimize for cost. So we do tiers and routing and non-uniform bandwidth and all that stuff. You lose out on worst-case performance but that’s the only way to get systems to scale.</p> <p>Turns out that this applies for everything — including tail latencies, jitter etc. The only way to guarantee that you will move a packet in a predictable timely manner is to explicitly carve out room for that packet on the path before sending it out. If you care about maintaining a high utilization or cost or scalability, this is bad.</p> <p>Given a guarantee, you can always make an argument that “we can relax this guarantee a little and get a lot more performance in the common case.” This is statistical multiplexing or oversubscription or optimistic concurrency control or whatever. These are incredibly important — these thoughtful relaxations are the reason that systems work and can be built for the prices that’re built for. But it is always nice to be explicit about what you are losing in return.</p> <h2 id="overloading-creates-more-risks">Overloading Creates More Risks</h2> <p>As the goal of this exercise was to understand CBFC, let us compare it with sender-driven mechanisms. Sender-driven congestion control overloads certain signals to infer network state — RTT and packet loss seem to be two of them. This works until something happens that changes the meaning of these numbers — bufferbloat being one example. An advantage, in principle, of CBFC is that it is explicitly communicating actual network state, so theoretically, it should be robust to some such issues. I do not know if that is actually realized in practice.</p> <p>This is in no way trashing sender-driven mechanisms. A good system is one that gets the job done — clean conscience and mathematical elegance are the domains of whiteboard hoggers. The end-to-end principle and this layered approach to TCP/IP has taken us a long long long way. Overloading those signals is the practical thing to do. (Also note that I’m conflating flow control with congestion control — CBFC is very much the former, but I guess it dictates and comes bundled with a different congestion control in IB with ECNs and injection throttling).</p> <h2 id="expectations-for-future-congestion-control">Expectations for Future Congestion Control</h2> <p>This post is a precursor to me trying to understand all the cool new things in datacenter fabrics — Omnipath, Slingshot, UltraEthernet, maybe Globally Scheduled Ethernet?</p> <p>If these arguments hold, none of these systems’ mechanisms will be perfect. There will always be cases where Bad Things$^{TM}$ happen. Part of the argument is that that may be okay, and an evaluation of their properties must be holistic.</p> <h2 id="some-degree-of-tuning-is-okay">Some Degree Of Tuning Is Okay?</h2> <ul> <li>I can not pick an outfit that works in all weathers. I am destined to keep tuning.</li> <li>Some tuning is worse than others. If I wore an outfit that had only one shoe, I would have to hop and keep swapping my shoe from one foot to the other.</li> <li>An outfit that covers the expected swings over one day is sufficient for an outfit.</li> <li>A bag that covers the expected swings over one week is sufficient for a short trip.</li> <li>A wardrobe that covers the expected swings over the year in Pittsburgh is sufficient for most other cases.</li> </ul> <p>What are the takeaways for systems?</p> <ul> <li>We are destined to tune. We should plan for that.</li> <li>It would be nice if we understood systems in terms of their coverage of design space.</li> <li>Nothing in this discussion rules out the existence of systems that cover strictly more of the design space than others. Probably someone has and/or will prove that certain aspects are pareto-optimal. But it’s still a small sheet over massive legs.</li> </ul> <h2 id="chasing-the-global-optima">Chasing The Global Optima</h2> <p>Network sharing and scheduling mechanisms are probably by far the most well-studied and deployed examples of a distributed system trying to construct/reach a global optimum, and both mathematical and practical arguments of the challenges that comes with.</p> <p>All systems would benefit from better decisions. Better decisions are enabled by a better view of the system state. But constructing this system state may be prohibitively expensive. It may also be a moving target. It will definitely have some uncertainty bounds because physics. It may also be possible that explicitly constructing the system state is not necessary — we can apparently go quite a bit with individual actors acting on simple rules. (Something about game theory and cooperative games comes to mind but I’m already wayyyy past my depth here).</p>]]></content><author><name></name></author><category term="networks"/><category term="networks"/><summary type="html"><![CDATA[Or, are we destined to tune?]]></summary></entry><entry><title type="html">Credits, Flow, and Congestion Control in Infiniband</title><link href="https://ankushja.in/blog/2024/credits-flow-congestion/" rel="alternate" type="text/html" title="Credits, Flow, and Congestion Control in Infiniband"/><published>2024-12-04T19:00:36+00:00</published><updated>2024-12-04T19:00:36+00:00</updated><id>https://ankushja.in/blog/2024/credits-flow-congestion</id><content type="html" xml:base="https://ankushja.in/blog/2024/credits-flow-congestion/"><![CDATA[<p>Things keep happening in datacenter networks. There (used to be) Ethernet, there’s Infiniband, Omnipath<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> (and its predecessors and successors), Slingshot<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, UltraEthernet<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, and now <em>Globally Scheduled Ethernet</em><sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. I don’t get why they all need to exist concurrently and this is an initial attempt at unraveling this for myself.</p> <p>Step 1 in this process is understanding Infiniband. Hyperscalers seem to like Infiniband and RDMA, but they also like their IP addresses and routing tables. RoCE and ROCEv2 were attempts to bridge the gaps, but they turned out to have problems. At this point, I know the talking points everyone rehashes — PFC storms, HoL blocking, deadlocks… but I don’t really understand any of it.</p> <p>From my understanding, all of the changes in RoCEv2 were necessitated because it is hard to get RDMA to work over lossy networks. (As an aside, it seems to me that there is no such thing as a truly lossless network, but you can build a pretty good illusion of one with two properties: 1. Reduce the likelihood of packet loss, and 2. Handle retransmissions transparently at some lower layer). I haven’t heard anyone complain about how flow control works in infiniband, so the key is maybe to understand it. This post is an attempt at that.</p> <h1 id="credit-based-flow-control-in-infiniband">Credit-based Flow Control in Infiniband</h1> <p>This is all based off slides here: <sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>.</p> <h2 id="virtual-lanes-and-service-levels">Virtual Lanes and Service Levels</h2> <p>Credits are maintained per-VL on each HCA. VLs/SLs are technically different but equivalent for now. They are priority classes — Infiniband supports up to 16. VL15 is reserved for subnet management traffic, while the others are available for regular data. The difference is that the application requests service levels, and the SL-VL mapping is a management decision handled presumably by the subnet manager.</p> <h2 id="basic-numbers">Basic Numbers</h2> <ul> <li>Each <code class="language-plaintext highlighter-rouge">Flow Control Block</code> is 64B. One 64B send requires one credit to be authorized.</li> <li>A VL will issue a maximum of 2048 credits, which translates to a 128KB receive buffer. <h2 id="cbfc-actual">CBFC Actual</h2> <p>There are 3 main terms: <code class="language-plaintext highlighter-rouge">ABR</code>, <code class="language-plaintext highlighter-rouge">FCCL</code>, <code class="language-plaintext highlighter-rouge">FCTBS</code>.</p> </li> <li><code class="language-plaintext highlighter-rouge">FCCL</code>: the credit limit (the max point up to which the sender has been authorized to send).</li> <li><code class="language-plaintext highlighter-rouge">ABR</code>: total blocks received at the receiver so far.</li> <li><code class="language-plaintext highlighter-rouge">FCTBS</code>: total blocks sent.</li> </ul> <p><code class="language-plaintext highlighter-rouge">FCTBS</code> - <code class="language-plaintext highlighter-rouge">ABR</code>: blocks in transit on the wire. <code class="language-plaintext highlighter-rouge">FCCL</code> - <code class="language-plaintext highlighter-rouge">FCTBS</code>: remaining credits for the sender. A send will be permitted if the size is smaller than this limit.</p> <h1 id="flow-control-vs-congestion-control">Flow Control vs Congestion Control</h1> <p>Flow control makes infiniband largely and inherently lossless. Rare occasions that cause packet corruption etc. may require retransmissions. The fabric will do retransmissions for you if <code class="language-plaintext highlighter-rouge">Reliable Connected</code> was requested. CBFC kicks in regardless of whether you use RC or UC or UD. I haven’t looked into how retransmissions for <code class="language-plaintext highlighter-rouge">RC</code> are managed.</p> <p>Infiniband also has congestion control on top of flow control. Why both need to exist is not entirely clear to me yet. What I do know is that IB employs some variant of <code class="language-plaintext highlighter-rouge">ECN (Explicit Congestion Notification)</code> to help detect congestion (<code class="language-plaintext highlighter-rouge">FECN</code> and <code class="language-plaintext highlighter-rouge">BECN</code>). I don’t know what the endpoints do in response to ECNs.</p> <h1 id="questions-for-myself">Questions For Myself</h1> <ul> <li>Why is it hard to retrofit CBFC on to ethernet?</li> <li>Why don’t all datacenter fabrics use CBFC?</li> <li>How does CBFC compare to loss-based congestion control. <h1 id="references">References</h1> </li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:2"> <p>What If Omni-Path Morphs Into The Best Ultra Ethernet?, , 2024, https://www.nextplatform.com/2024/06/26/what-if-omni-path-morphs-into-the-best-ultra-ethernet/ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Cray’s Slingshot Interconnect Is At The Heart Of HPE’s HPC And AI Ambitions, , 2022, https://www.nextplatform.com/2022/01/31/crays-slingshot-interconnect-is-at-the-heart-of-hpes-hpc-and-ai-ambitions/ <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1"> <p>UltraEthernet: Overview of and Motivation for the Forthcoming Ultra Ethernet Consortium Specification, , , <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>Whitepaper on Globally Scheduled Ethernet, , 2024, https://regmedia.co.uk/2024/11/26/china_mobile_gse_whitepaper.pdf <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>Infiniband Credit-Based Link-Layer Flow-Control, , 2014, https://www.ieee802.org/1/files/public/docs2014/new-dcb-crupnicoff-ibcreditstutorial-0314.pdf <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="systems"/><category term="networks"/><summary type="html"><![CDATA[Notes on CBFC, lossless fabrics, receiver-driven congestion control etc.]]></summary></entry><entry><title type="html">Notes on Web Pages and CMU Infra</title><link href="https://ankushja.in/blog/2024/cmu-web-pages.md/" rel="alternate" type="text/html" title="Notes on Web Pages and CMU Infra"/><published>2024-11-29T18:11:12+00:00</published><updated>2024-11-29T18:11:12+00:00</updated><id>https://ankushja.in/blog/2024/cmu-web-pages.md</id><content type="html" xml:base="https://ankushja.in/blog/2024/cmu-web-pages.md/"><![CDATA[<p>Infra for hosting web pages on CMU web servers is a little complicated and confusing, and I waste some time figuring it out every time I need to change something. These are notes to make my life easier in the future, and may be relevant for others. This is from the perspective of an ECE student, adapt to your needs.</p> <h2 id="afs-cells">AFS Cells</h2> <p>I get to have a home directory on two CMU cells – <code class="language-plaintext highlighter-rouge">andrew.cmu.edu</code> and <code class="language-plaintext highlighter-rouge">ece.cmu.edu</code>. I don’t know how to discover all the cells you have a home directory in.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh ankushj@unix.andrew.cmu.edu # give them your andrew password
$ pwd
/afs/andrew.cmu.edu/usr18/ankushj
$ cd /afs/ece.cmu.edu/usr/ankushj
$ ls
Permission denied
$ aklog ece.cmu.edu
$ ls
top_secret_infiniband_content.txt
$ tokens
&lt;all the cells I'm authenticated for&gt;

# random AFS commands
$ fs listaliases
$ fs listquota
</code></pre></div></div> <h2 id="hosting-on-ece">Hosting on ECE</h2> <p>This is straightforward. Your <code class="language-plaintext highlighter-rouge">public_html</code> is available on <code class="language-plaintext highlighter-rouge">users.ece.cmu.edu/~ankushj</code></p> <p>ECE also respects <code class="language-plaintext highlighter-rouge">.htaccess</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -a /path/to/ece/homedir/public_html
.htaccess index.html
</code></pre></div></div> <h2 id="hosting-on-andrew">Hosting on Andrew</h2> <p>This is more confusing. Target address is <code class="language-plaintext highlighter-rouge">https://www.andrew.cmu.edu/user/ankushj/</code>.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -a /path/to/andrew/homedir/www
.htaccess index.html
</code></pre></div></div> <ol> <li>Andrew does not use <code class="language-plaintext highlighter-rouge">.htaccess</code>.</li> <li><code class="language-plaintext highlighter-rouge">index.html</code> is not automatically available. You have to go to <code class="language-plaintext highlighter-rouge">https://www.andrew.cmu.edu/server/publish.html</code>, type your username, hit publish, and the data gets copied to some “real hosting destination”.</li> <li><code class="language-plaintext highlighter-rouge">.unpublish</code> is a thing<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></li> </ol> <h2 id="redirects">Redirects</h2> <p>On ECE, I can set up a HTTP-level redirect via <code class="language-plaintext highlighter-rouge">.htaccess</code>.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RedirectMatch 301 ^/$ https://my.real.website
</code></pre></div></div> <p>I previously tried the following, but it would redirect to <code class="language-plaintext highlighter-rouge">https://my.real.website~ankushj</code>. I am pretty sure that ChatGPT is just giving me a stupid redirect command, but the version above works and the version below does not — that’ll have to do for now.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Redirect 301 / https://my.real.website
</code></pre></div></div> <p>From Andrew, I had to set up a HTTP redirect.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
        &lt;meta http-equiv="refresh" content="0; url=https//my.real.website"&gt;
            &lt;title&gt;Redirecting...&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
        &lt;p&gt;If you are not redirected, &lt;a href="https://my.real.website"&gt;click here&lt;/a&gt;.&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre></div></div> <h2 id="troubleshooting">Troubleshooting</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Permissions
$ fs sa www system:anyuser rl
</code></pre></div></div> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://www.cmu.edu/computing/services/comm-collab/websites/user-course-web/how-to/publish.html <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="web"/><category term="web"/><summary type="html"><![CDATA[Dealing with AFS, cells, permissions etc.]]></summary></entry><entry><title type="html">Inscrutable Coredump v. Unmoveable Grad Student</title><link href="https://ankushja.in/blog/2024/inscrutable-core-dump/" rel="alternate" type="text/html" title="Inscrutable Coredump v. Unmoveable Grad Student"/><published>2024-07-09T19:24:13+00:00</published><updated>2024-07-09T19:24:13+00:00</updated><id>https://ankushja.in/blog/2024/inscrutable-core-dump</id><content type="html" xml:base="https://ankushja.in/blog/2024/inscrutable-core-dump/"><![CDATA[<p>Situation: we have a core dump that is shy to reveal its inner workings. The goal is to extract some more information from this core dump, using fancier analyses. The core dump is 1.2GB in size, so I know that there is some insight in there, it is just buried.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) bt</span><span class="w">
</span>#0<span class="w">  </span><span class="mh">0x00007fbaa4653c30</span><span class="w"> </span>in<span class="w"> </span>??<span class="w"> </span><span class="p">()</span><span class="w">
</span>#1<span class="w">  </span><span class="mh">0x0000000000000000</span><span class="w"> </span>in<span class="w"> </span>??<span class="w"> </span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>Since we have an intact <code class="language-plaintext highlighter-rouge">$pc</code> (which refers to <code class="language-plaintext highlighter-rouge">%rip</code>), we can figure out the instructions it was executing.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) disassemble $pc-20,$pc+20</span><span class="w">
</span>Dump<span class="w"> </span>of<span class="w"> </span>assembler<span class="w"> </span>code<span class="w"> </span>from<span class="w"> </span><span class="mh">0x7fbaa4653c1c</span><span class="w"> </span>to<span class="w"> </span><span class="mh">0x7fbaa4653c44</span>:<span class="w">
   </span><span class="mh">0x00007fbaa4653c1c</span>:<span class="w">  </span>add<span class="w">    </span><span class="nv">%al</span><span class="p">,(</span><span class="nv">%rax</span><span class="p">)</span><span class="w">
   </span><span class="mh">0x00007fbaa4653c1e</span>:<span class="w">  </span>jmp<span class="w">    </span><span class="mh">0x7fbaa4653a51</span><span class="w">
   </span><span class="mh">0x00007fbaa4653c23</span>:<span class="w">  </span>nopl<span class="w">   </span><span class="mh">0x0</span><span class="p">(</span><span class="nv">%rax</span><span class="p">,</span><span class="nv">%rax</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="w">
   </span><span class="mh">0x00007fbaa4653c28</span>:<span class="w">  </span>mov<span class="w">    </span><span class="mh">0x98</span><span class="p">(</span><span class="nv">%r12</span><span class="p">),</span><span class="nv">%rax</span><span class="w">
</span>=&gt;<span class="w"> </span><span class="mh">0x00007fbaa4653c30</span>:<span class="w">  </span>cmpb<span class="w">   </span>$0x48,(%rax)<span class="w">
   </span><span class="mh">0x00007fbaa4653c33</span>:<span class="w">  </span>jne<span class="w">    </span><span class="mh">0x7fbaa4653b90</span><span class="w">
   </span><span class="mh">0x00007fbaa4653c39</span>:<span class="w">  </span>movabs<span class="w"> </span>$0x50f0000000fc0c7,%rdx<span class="w">
   </span><span class="mh">0x00007fbaa4653c43</span>:<span class="w">  </span>cmp<span class="w">    </span><span class="nv">%rdx</span><span class="p">,</span><span class="mh">0x1</span><span class="p">(</span><span class="nv">%rax</span><span class="p">)</span><span class="w">
</span>End<span class="w"> </span>of<span class="w"> </span>assembler<span class="w"> </span>dump.<span class="w">
</span></code></pre></div></div> <p>Let us inspect the registers.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) info reg</span><span class="w">
</span>rax<span class="w">            </span><span class="mh">0x323338342034342e</span><span class="w">  </span><span class="mi">3617296722238387246</span><span class="w">
</span>rbx<span class="w">            </span><span class="mh">0x7fff678ebb50</span><span class="w">      </span><span class="mi">140734930795344</span><span class="w">
</span>rcx<span class="w">            </span><span class="mh">0x7fba82086120</span><span class="w">      </span><span class="mi">140439022231840</span><span class="w">
</span>rdx<span class="w">            </span><span class="mh">0x1</span><span class="w">                 </span><span class="mi">1</span><span class="w">
</span>rsi<span class="w">            </span><span class="mh">0x1</span><span class="w">                 </span><span class="mi">1</span><span class="w">
</span></code></pre></div></div> <p>Okay so our <code class="language-plaintext highlighter-rouge">%rax</code> was clearly a gibberish address, no wonder dereferencing it failed. Now the question is what source file/line was mapped to <code class="language-plaintext highlighter-rouge">$pc</code>. ChatGPT says that the following can work:</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) list *$pc</span><span class="w">
</span>&lt;no<span class="w"> </span>output&gt;<span class="w">
</span><span class="gp">(gdb) info symbol $pc</span><span class="w">
</span>No<span class="w"> </span>symbol<span class="w"> </span>matches<span class="w"> </span>$pc.<span class="w">
</span></code></pre></div></div> <p>ChatGPT also says that we can also dereference addresses using these, but first we need to know what library is laid out in our memory, and at what offset. Noting these down for later.</p> <div class="language-session highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$ addr2line -e /path/to/your/executable 0xADDRESS</span><span class="w">
</span><span class="gp">$ objdump -d -S /path/to/your/executable</span><span class="w">
</span></code></pre></div></div> <p>Some more useful information, saving for later.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) info frame 0</span><span class="w">
</span>Stack<span class="w"> </span>frame<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf8</span>:<span class="w">
 </span>rip<span class="w"> </span>=<span class="w"> </span><span class="mh">0x7fbaa4653c30</span>;<span class="w"> </span>saved<span class="w"> </span>rip<span class="w"> </span>=<span class="w"> </span><span class="mh">0x0</span><span class="w">
 </span>called<span class="w"> </span>by<span class="w"> </span>frame<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebb00</span><span class="w">
 </span>Arglist<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebae8</span><span class="p">,</span><span class="w"> </span>args:<span class="w">
 </span>Locals<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebae8</span><span class="p">,</span><span class="w"> </span>Previous<span class="w"> </span>frame's<span class="w"> </span>sp<span class="w"> </span>is<span class="w"> </span><span class="mh">0x7fff678ebaf8</span><span class="w">
 </span>Saved<span class="w"> </span>registers:<span class="w">
  </span>rip<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf0</span><span class="w">
</span><span class="gp">(gdb) info frame 1</span><span class="w">
</span>Stack<span class="w"> </span>frame<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebb00</span>:<span class="w">
 </span>rip<span class="w"> </span>=<span class="w"> </span><span class="mh">0x0</span>;<span class="w"> </span>saved<span class="w"> </span>rip<span class="w"> </span>=<span class="w"> </span><span class="mh">0x0</span><span class="w">
 </span>caller<span class="w"> </span>of<span class="w"> </span>frame<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf8</span><span class="w">
 </span>Arglist<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf0</span><span class="p">,</span><span class="w"> </span>args:<span class="w">
 </span>Locals<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf0</span><span class="p">,</span><span class="w"> </span>Previous<span class="w"> </span>frame's<span class="w"> </span>sp<span class="w"> </span>is<span class="w"> </span><span class="mh">0x7fff678ebb00</span><span class="w">
 </span>Saved<span class="w"> </span>registers:<span class="w">
  </span>rip<span class="w"> </span>at<span class="w"> </span><span class="mh">0x7fff678ebaf8</span><span class="w">
</span><span class="gp">(gdb) info frame 2</span><span class="w">
</span>No<span class="w"> </span>frame<span class="w"> </span>at<span class="w"> </span>level<span class="w"> </span><span class="mi">2</span>.<span class="w">
</span></code></pre></div></div> <p>Let’s look at shared memory mappings now.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) info shared</span><span class="w">
</span>No<span class="w"> </span>shared<span class="w"> </span>libraries<span class="w"> </span>loaded<span class="w"> </span>at<span class="w"> </span>this<span class="w"> </span>time.<span class="w">
</span><span class="gp">(gdb) info proc mappings</span><span class="w">
</span>Mapped<span class="w"> </span>address<span class="w"> </span>spaces:<span class="w">

          </span>Start<span class="w"> </span>Addr<span class="w">           </span>End<span class="w"> </span>Addr<span class="w">       </span>Size<span class="w">     </span>Offset<span class="w"> </span>objfile<span class="w">
      </span><span class="mh">0x5585a6034000</span><span class="w">     </span><span class="mh">0x5585a604b000</span><span class="w">    </span><span class="mh">0x17000</span><span class="w">        </span><span class="mh">0x0</span><span class="w"> </span>/some/bin<span class="w">
      </span><span class="mh">0x5585a604b000</span><span class="w">     </span><span class="mh">0x5585a64ef000</span><span class="w">   </span><span class="mh">0x4a4000</span><span class="w">    </span><span class="mh">0x17000</span><span class="w"> </span>/some/bin<span class="w">
      </span><span class="mh">0x5585a64ef000</span><span class="w">     </span><span class="mh">0x5585a6582000</span><span class="w">    </span><span class="mh">0x93000</span><span class="w">   </span><span class="mh">0x4bb000</span><span class="w"> </span>/some/bin<span class="w">
      </span><span class="mh">0x7fba3ad28000</span><span class="w">     </span><span class="mh">0x7fba3c000000</span><span class="w">  </span><span class="mh">0x12d8000</span><span class="w">        </span><span class="mh">0x0</span><span class="w"> </span>/dev/shm/...<span class="w">
      </span><span class="mh">0x7fba40d29000</span><span class="w">     </span><span class="mh">0x7fba40f41000</span><span class="w">   </span><span class="mh">0x218000</span><span class="w">        </span><span class="mh">0x0</span><span class="w"> </span>/dev/shm/...<span class="w">
</span></code></pre></div></div> <p>Alright, getting somewhere. I have no idea why <code class="language-plaintext highlighter-rouge">info shared</code> failed but <code class="language-plaintext highlighter-rouge">info proc mappings</code> did not. We want to find a mapping around the address <code class="language-plaintext highlighter-rouge">0x00007fbaa4653c30</code>.</p> <p>Found something.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mh">0x7fbaa4647000</span><span class="w">     </span><span class="mh">0x7fbaa4659000</span><span class="w">    </span><span class="mh">0x12000</span><span class="w">     </span><span class="mh">0x3000</span><span class="w"> </span>/usr/lib/x86_64-linux-gnu/libgcc_s.so.1<span class="w">
</span></code></pre></div></div> <p>The difference between the base address and our <code class="language-plaintext highlighter-rouge">$pc</code> is <code class="language-plaintext highlighter-rouge">0xcc30</code>. Add the offset <code class="language-plaintext highlighter-rouge">0x3000</code> to get <code class="language-plaintext highlighter-rouge">0xfc30</code>.</p> <div class="language-session highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$ addr2line -e /usr/lib/x86_64-linux-gnu/libgcc_s.so.1 0xfc30</span><span class="w">
</span>??;0<span class="w">
</span></code></pre></div></div> <p>Okay well thx.</p> <div class="language-session highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$ objdump -d -S /usr/lib/x86_64-linux-gnu/libgcc_s.so.1</span><span class="w">
</span>...<span class="w">
</span>fc30:<span class="w">       </span>80<span class="w"> </span>38<span class="w"> </span>48<span class="w">                </span>cmpb<span class="w">   </span>$0x48,(%rax)<span class="w">
    </span>fc33:<span class="w">       </span>0f<span class="w"> </span>85<span class="w"> </span>57<span class="w"> </span>ff<span class="w"> </span>ff<span class="w"> </span>ff<span class="w">       </span>jne<span class="w">    </span>fb90<span class="w"> </span>&lt;_Unwind_GetTextRelBase@@GCC_3.0+0xe40&gt;<span class="w">
    </span>fc39:<span class="w">       </span>48<span class="w"> </span>ba<span class="w"> </span>c7<span class="w"> </span>c0<span class="w"> </span>0f<span class="w"> </span>00<span class="w"> </span>00<span class="w">    </span>movabs<span class="w"> </span>$0x50f0000000fc0c7,%rdx<span class="w">
    </span>...<span class="w">
</span></code></pre></div></div> <p>Okay this wasn’t super useful. This is just some GCC unwinding utility function after a segfault. At this point, I just decide to examine the entire stack.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) x/128xg 0x7fff678eba00</span><span class="w">
</span>...<span class="w">
</span><span class="mh">0x7fff678eba00</span>:<span class="w"> </span><span class="mh">0x00007fba82086040</span><span class="w">      </span><span class="mh">0x00007fbaa465000b</span><span class="w">
</span><span class="mh">0x7fff678eba10</span>:<span class="w"> </span><span class="mh">0x000000000000002e</span><span class="w">      </span><span class="mh">0x0000000000000000</span><span class="w">
</span><span class="mh">0x7fff678eba20</span>:<span class="w"> </span><span class="mh">0x0000000000000000</span><span class="w">      </span><span class="mh">0x0000000000000000</span><span class="w">
</span><span class="mh">0x7fff678eba30</span>:<span class="w"> </span><span class="mh">0x00007fff678eced8</span><span class="w">      </span><span class="mh">0xb741446eb7f0e800</span><span class="w">
</span><span class="mh">0x7fff678eba40</span>:<span class="w"> </span><span class="mh">0x00007fff678eceb0</span><span class="w">      </span><span class="mh">0x00007fff678ebb50</span><span class="w">
</span><span class="mh">0x7fff678eba50</span>:<span class="w"> </span><span class="mh">0x00007fff678ebbf8</span><span class="w">      </span><span class="mh">0x00007fff678ebb50</span><span class="w">
</span><span class="mh">0x7fff678eba60</span>:<span class="w"> </span><span class="mh">0x00007fff678ebe00</span><span class="w">      </span><span class="mh">0x323338342034342d</span><span class="w">
</span>...<span class="w">
</span></code></pre></div></div> <p>Some patterns start to emerge. All values starting with <code class="language-plaintext highlighter-rouge">0x7fff</code> are pointers to things on the stack. Things in the range of <code class="language-plaintext highlighter-rouge">0x7fbaa..</code> are probably related to instructions. We can also see the junk value <code class="language-plaintext highlighter-rouge">0x3233</code> that was implicated in the segfault.</p> <h2 id="two-hours-later-">Two hours later …</h2> <p>My approach was to examine the stack visually, find pointers with prefixes that I knew to map to code I wore, and try and dereference them to get an idea of where my program was when it crashed.</p> <p>This is doable, but it is not as straightforward as you might think. The <em>why</em> requires going into how ELF binaries/shared libraries are loaded in the memory.</p> <ol> <li>There is a <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;/map</code> corresponding to <code class="language-plaintext highlighter-rouge">info proc mappings</code> that we saw earlier.</li> <li>Each ELF file is divided into segments, which are further divided into sections. Mapping happens at the granularity of a segment.</li> <li>The mapped segment will have a different offset than the on-disk segment. This may have something to do with alignment and/or ASLR requirements. But the segment sizes are also different for me, between what is reported by gdb, and what is shown by <code class="language-plaintext highlighter-rouge">readelf/objdump</code>.</li> </ol> <p>As a result, I was unable to map symbol addresses from the core dump to symbols in libraries effectively. There is theoretically no reason why gdb should not be able to do this automatically, and it does, for more benign cases. But it does not seem to load the shared libraries for me for this particular crash.</p> <h2 id="wait-">Wait …</h2> <p>Okay, I ran gdb with this specific sequence, and suddenly it chose to load shared libraries.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$<span class="w"> </span>gdb<span class="w">
</span><span class="gp">(gdb) set auto-solib-add off # do not auto-load solibs</span><span class="w">
</span><span class="gp">(gdb) set substitute-path /dev/shm /dev/null # something for shm maps</span><span class="w">
</span><span class="gp">(gdb) set solib-search-path /path/to/lib</span><span class="w">
</span><span class="gp">(gdb) file /path/to/my/binary</span><span class="w">
</span><span class="gp">(gdb) target core /path/to/core-file</span><span class="w">
</span><span class="gp">(gdb) info sharedlibrary</span><span class="w">
</span><span class="gp">(gdb) info sharedlibrary</span><span class="w">
</span>From<span class="w">                </span>To<span class="w">                  </span>Syms<span class="w"> </span>Read<span class="w">   </span>Shared<span class="w"> </span>Object<span class="w"> </span>Library<span class="w">
</span><span class="mh">0x00007fbaa4e98350</span><span class="w">  </span><span class="mh">0x00007fbaa4eaccd1</span><span class="w">  </span>No<span class="w">          </span>/lib/libx.so<span class="w"> 
</span><span class="mh">0x00007fbaa4e21a00</span><span class="w">  </span><span class="mh">0x00007fbaa4e72bc9</span><span class="w">  </span>No<span class="w">          </span>/lib/liby.so<span class="w">
</span>...<span class="w">
</span><span class="gp">(gdb) sharedlibrary /path/to/libmycode.so</span><span class="w">
</span>Reading<span class="w"> </span>symbols<span class="w"> </span>from<span class="w"> </span>...<span class="w">
</span></code></pre></div></div> <p>I have no idea which of the above did the trick. Consider it a magic sequence of commands for now.</p> <p>The game plan now is to go through the stack with <code class="language-plaintext highlighter-rouge">x/64xg $pc</code> and beyond to look for familiar addresses and try to resolve them via the symbol table. I tried a bunch of random symbols, and finally hit jackpot.</p> <div class="language-gdb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(gdb) info symbol 0x00005585a6470c80</span><span class="w">
</span>Serialize[...]<span class="w"> </span>in<span class="w"> </span>section<span class="w"> </span>.text<span class="w"> </span>of<span class="w"> </span>/my/binary<span class="w">
</span></code></pre></div></div> <p>It was a buffer overflow in a serialization routine.</p> <h2 id="conclusions">Conclusions</h2> <p>The battle between you and a coy-acting core dump is a battle of wills. Do not blink.</p>]]></content><author><name></name></author><category term="systems"/><category term="gdb"/><summary type="html"><![CDATA[On the applications of fuzzy human pattern matching to extract secrets from a corrupted core dump in the age of trillion parameter AI]]></summary></entry><entry><title type="html">Using Perf Probes to Intercept Variables</title><link href="https://ankushja.in/blog/2024/perf-inspect-vars/" rel="alternate" type="text/html" title="Using Perf Probes to Intercept Variables"/><published>2024-05-29T22:12:07+00:00</published><updated>2024-05-29T22:12:07+00:00</updated><id>https://ankushja.in/blog/2024/perf-inspect-vars</id><content type="html" xml:base="https://ankushja.in/blog/2024/perf-inspect-vars/"><![CDATA[<p><a href="">Part 1</a> and <a href="">Part 2</a> of this series at the respective links.</p> <p>This is a case study in using perf probes to monitor the values of some variable from a process, without having to modify its code.</p> <p>We are interested in monitoring the number of objects allocated by a library called <code class="language-plaintext highlighter-rouge">psm</code>. We know that they are available in a function called <code class="language-plaintext highlighter-rouge">psmmpool_get</code>. The object available there is called <code class="language-plaintext highlighter-rouge">mp</code> and it has members <code class="language-plaintext highlighter-rouge">mp_num_obj</code> and <code class="language-plaintext highlighter-rouge">mp_num_obj_inuse</code> that we want to monitor.</p> <p>First, set up your permissions.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">sudo </span>addgroup tracing
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> ankush tracing
newgrp tracing <span class="c"># "activate" group without having to log out</span>
<span class="nb">sudo </span>mount <span class="nt">-o</span> remount,mode<span class="o">=</span>755 /sys/kernel/debug
<span class="nb">sudo </span>mount <span class="nt">-o</span> remount,mode<span class="o">=</span>755 /sys/kernel/debug/tracing
<span class="nb">echo </span>0 | <span class="nb">sudo tee</span> /proc/sys/kernel/kptr_restrict
<span class="nb">echo</span> <span class="nt">-1</span> | <span class="nb">sudo tee</span> /proc/sys/kernel/perf_event_paranoid
<span class="nb">sudo chown </span>root:tracing /sys/kernel/debug/tracing/uprobe_events
<span class="nb">sudo chmod </span>g+rw /sys/kernel/debug/tracing/uprobe_events
</code></pre></div></div> <p>Next, we execute these commands to explore the available perf points and tap into them</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PSM</span><span class="o">=</span>/path/to/libpsm_infinipath.so

perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="nt">--funcs</span> <span class="c"># lists probe-able functions</span>
perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="nt">--funcs</span> | <span class="nb">grep </span>psmi_mpool_get <span class="c"># this is present</span>
perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="nt">-L</span> psmi_mpool_get <span class="c"># view the source for this function</span>
perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="nt">--vars</span> psmi_mpool_get <span class="c"># view available vars here.</span>

<span class="c"># mp is available in psmi_mpool_get</span>

<span class="c"># need mp-&gt;mp_num_obj and mp-&gt;mp_num_obj_inuse</span>
perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="s1">'psm:nobj=psmi_mpool_get mp-&gt;mp_num_obj'</span>
perf probe <span class="nt">-x</span> <span class="nv">$PSM</span> <span class="s1">'psm:nused=psmi_mpool_get mp-&gt;mp_num_obj_inuse'</span>
</code></pre></div></div> <p>List active probes to confirm that they got added</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ perf probe -l
  psm:nobj             (on psmi_mpool_get@src/psm/psm_mpool.c in ...&gt;
  psm:nused            (on psmi_mpool_get@src/psm/psm_mpool.c in ...&gt;
</code></pre></div></div> <p>Start recording for all probes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perf record <span class="nt">-e</span> psm:nobj <span class="nt">-e</span> psm:nused <span class="nt">-a</span>
<span class="c"># run code from a different tab</span>
^C
perf script <span class="c"># this will show us what was emitted</span>
</code></pre></div></div> <p>Output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> bin-using-psm 381026 [002] 417634.233519: psm:nused: (7f9980230350) mp_num_obj_inuse=0x0
 bin-using-psm 381027 [003] 417634.233519: psm:nused: (7fcc080f5350) mp_num_obj_inuse=0x0
 bin-using-psm 381028 [004] 417634.233519: psm:nused: (7fc05836e350) mp_num_obj_inuse=0x0
 bin-using-psm 381027 [003] 417634.233522:  psm:nobj: (7fcc080f5350) mp_num_obj=0x400
 bin-using-psm 381028 [004] 417634.233522:  psm:nobj: (7fc05836e350) mp_num_obj=0x400
 bin-using-psm 381026 [002] 417634.233522:  psm:nobj: (7f9980230350) mp_num_obj=0x400
 bin-using-psm 381029 [005] 417634.233523: psm:nused: (7fa818f52350) mp_num_obj_inuse=0x0
 bin-using-psm 381029 [005] 417634.233525:  psm:nobj: (7fa818f52350) mp_num_obj=0x400
 bin-using-psm 381030 [006] 417634.233525: psm:nused: (7f69a6c35350) mp_num_obj_inuse=0x0
 bin-using-psm 381030 [006] 417634.233527:  psm:nobj: (7f69a6c35350) mp_num_obj=0x400
</code></pre></div></div> <p>Bingo — we have the data. Other things:</p> <ol> <li>We can point <code class="language-plaintext highlighter-rouge">perf record</code> to a particular PID to record its values.</li> <li>We can process the data via say python using <code class="language-plaintext highlighter-rouge">perf script -g python</code></li> </ol> <p>Something like this will show the number of occurrences in real time, but AFAIK, can not show data embedded in the probe.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perf <span class="nb">stat</span> <span class="nt">-a</span> <span class="nt">-e</span> psm:nobj <span class="nt">-I</span> 1000
</code></pre></div></div> <h3 id="references">References</h3>]]></content><author><name></name></author><category term="perf"/><category term="perf"/><summary type="html"><![CDATA[Intercept process variables without having to modify it]]></summary></entry><entry><title type="html">On Intelligence: Assessment and Directions</title><link href="https://ankushja.in/blog/2024/llm-intelligence/" rel="alternate" type="text/html" title="On Intelligence: Assessment and Directions"/><published>2024-05-15T05:53:05+00:00</published><updated>2024-05-15T05:53:05+00:00</updated><id>https://ankushja.in/blog/2024/llm-intelligence</id><content type="html" xml:base="https://ankushja.in/blog/2024/llm-intelligence/"><![CDATA[<p>So the topic for this post is a bit of a departure from my usual mucking in the sewers that modern systems are. I have been fascinated by LLMs, and what they are missing enroute to “AGI”. This is a collection of some of those thoughts that have been brewing over the past few months. I am not particularly familiar with the related work in this area, and this is mostly coming from first principles, with some random inputs from here and there folded in.</p> <p>I have no calibration for how much of this is just me reiterating what is already known and well-understood, or obvious, or deeply flawed.</p> <h2 id="llms-as-currently-designed-can-not-be-reasoning">LLMs, As Currently Designed, Can Not Be Reasoning</h2> <p>This idea emerged in my head in the early days of GPT-3. Its prowess was shocking, and everyone started reassessing everything (<em>autogaslighting</em>). And then we started noticing a million ways to “crack” it, relaxed a little, and things got better with GPT-4/4o.</p> <p>This intuition is simple — LLMs generate text at a constant token rate, and an intelligent agent simply can not do that. For a simple query, an intelligent agent is able to respond instantly, while for a complex query, it needs to go and <em>think</em> and <em>plan</em> a response. There can not be any upper bound to the amount of time this takes.</p> <p>LLMs seem to encode some constant amount of capability. It is perfectly okay for a new (yet not fully optimized architecture that is on the right track) to take more time than necessary for a simple query, but an architecture that can not take an infinite time can not be reasoning.</p> <p>I have come across the term “out-of-distribution” to describe hallucination etc, and I think it offers a good way to think about LLMs — they form a distribution over all that is known, and are able to generate samples from that distribution. For why this does not amount to reasoning, read on and I think you will have a better idea of how this is going.</p> <h2 id="llms-as-a-snapshot-of-intuition">LLMs as a Snapshot of Intuition</h2> <p>I think LLMs are a lot more similar to <em>intuition</em>. Intuition is the mechanism by which we generate instant responses — these can be right, or wrong. I say <em>a snapshot of intuition</em> because human intuition constantly refines itself, and model weights are static.</p> <p>Intuition is a different property from <em>planning</em> and <em>reasoning</em>. I will describe this more in the “human learning” section — I feel that there are circular dependencies here and I can not seem to figure out a great order to put this in.</p> <p>Drawing an equivalence between LLMs and intuition has two interesting corollaries:</p> <ol> <li>It provides an explanation for why we are building massive and expensive models. This is somewhat similar to building an intuition so strong that you make up for an inability to reason.</li> <li>It maps them to a component of the grand architecture of “human cognitive ability”. This would imply that while we (as the human race) have not yet figured out the entire architecture, this is fundamentally similar to a building block.</li> </ol> <p>I do feel that the fundamental approach to synthesizing “capability” by “exposing” a neural network to lots of data is valid and powerful. This is in contrast to coding and program synthesis, where “capability” is hand-crafted line-by-line. This is not to say that the latter approaches are not valid, but that the former allows for capabilities the latter does not (and vice-versa), and seems to mirror aspects of the elusive human intelligence.</p> <h3 id="a-model-of-human-intelligence">A Model of Human Intelligence</h3> <p>I think my point should make a lot more sense if I explain my model of human intelligence.</p> <p>Say a “rational” human being is asked a question (say a physics situation involving some bodies). This is the sequence of steps they take:</p> <ol> <li>They have an initial intuition of how they expect the system to behave.</li> <li>They try to formally solve it by modeling it as a set of equations, solve the equations, and get a result.</li> <li>Let us pretend that they get a result that they believe is “counterintuitive”. They recheck their math and if it holds, accept that “while my intuition suggests otherwise, this must be true.”</li> <li>Over time, their intuition internalizes what the math says, and a year later if they are asked a similar question, their instinct is a lot closer.</li> </ol> <p>Now we can define <em>reasoning</em> and <em>planning</em>.</p> <ul> <li><em>Reasoning</em> is just modeling a situation as a set of logical statements, and them symbolically validating those statements.</li> <li><em>Planning</em> is decomposing a problem into a series of subproblems, possibly recursively, and solving the smaller subproblems, and composing all the leaf nodes into a solution for the bigger problem. <ul> <li>The “plan” for an “agent” can include “actions”. Over the course of trying to solve a problem, they may ask other agents for assistance, build machines, or take any other action within its scope.</li> </ul> </li> </ul> <p>“Agents” also “meta-reason” and “meta-intuit”. They think about whether their plan is logically valid or not. They have different preference profiles over the set of candidate actions. (Some are more likely to approach other agents, others may prefer choosing goals that avoid interactions etc.) These heuristics lead to emergent traits such as personality, extroversion, neuroticism etc.</p> <h2 id="a-lesson-from-flight">A Lesson from Flight</h2> <p>Now how does all this apply to how to build such capabilities? That is the quadrillion dollar question.</p> <p>How humans cracked flight is, in my opinion, an instructive parallel. That birds could fly established that flight is possible. However, the mechanics we use to actually fly are very different — our aircraft do not flap their wings. We probably still can not build a meaningful wing-flapping aircraft. That in no way limits our ability to leverage aviation and structure our entire economies around that capability being commonplace.</p> <p>The same holds for LLMs. They need <em>reasoning</em> and <em>planning</em>, but the way they achieve these capabilities may be very different from the way we evolved. Life evolved as a self-sustaining Minimal Viable Product that could mutate to gain capabilities. We may be able to stitch together a “self-sustaining cognitive loop” directly from larger pieces. Or we may find that the easier way to do it is to build simpler agents and let them self-mutate to acquire capability.</p> <p>I think a variety of architectures are possible — autonomous and not. It is not necessary that they are conscious, “alive”, have free will, have feelings, etc.</p> <h2 id="a-lesson-from-organic-chemistry">A Lesson from Organic Chemistry</h2> <p>Before the 18th century, we believed that certain molecules, associated with living beings, had an inherent life force, and therefore could not be synthesized artificially. From Wikipedia:</p> <blockquote> <p>In 1828 <a href="https://en.wikipedia.org/wiki/Friedrich_W%C3%B6hler" title="Friedrich Wöhler">Friedrich Wöhler</a> produced the <em>organic</em> chemical <a href="https://en.wikipedia.org/wiki/Urea" title="Urea">urea</a> (carbamide), a constituent of <a href="https://en.wikipedia.org/wiki/Urine" title="Urine">urine</a>, from <em>inorganic</em> starting materials (the salts <a href="https://en.wikipedia.org/wiki/Potassium_cyanate" title="Potassium cyanate">potassium cyanate</a> and <a href="https://en.wikipedia.org/wiki/Ammonium_sulfate" title="Ammonium sulfate">ammonium sulfate</a>), in what is now called the <a href="https://en.wikipedia.org/wiki/W%C3%B6hler_synthesis" title="Wöhler synthesis">Wöhler synthesis</a>.</p> </blockquote> <p>It was then that we realized that organic molecules have nothing inherently different from others. They could be synthesized, hacked, and novel ones could be created to trick our bodies into working a certain way.</p> <p>I think that to the extent we think about intelligence as something inherent to humans, we will shortly be proven wrong. I also think that it will be less of a breakthrough moment than a series of continual improvements of the kind we are currently in the midst of — deeply shaping the way we work and organize human productivity along the way.</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="llm"/><category term="aligners"/><summary type="html"><![CDATA[How far have LLMs gotten us? What remains? What will "the rest" take?]]></summary></entry><entry><title type="html">An Exercise in Debugging When the Kernel Is Implicated</title><link href="https://ankushja.in/blog/2024/quis-custodiet/" rel="alternate" type="text/html" title="An Exercise in Debugging When the Kernel Is Implicated"/><published>2024-04-15T20:22:10+00:00</published><updated>2024-04-15T20:22:10+00:00</updated><id>https://ankushja.in/blog/2024/quis-custodiet</id><content type="html" xml:base="https://ankushja.in/blog/2024/quis-custodiet/"><![CDATA[<p>Situation: an MPI application (10s of processes) seems to freeze after completion. Once it freezes, I can’t attach to any of its processes via <code class="language-plaintext highlighter-rouge">gdb -p</code> (that also freezes). I can’t <code class="language-plaintext highlighter-rouge">SIGTERM</code> or even <code class="language-plaintext highlighter-rouge">SIGKILL</code> the processes. <code class="language-plaintext highlighter-rouge">ps aux</code> (my standard set of ps flags) shows the processes in either an <code class="language-plaintext highlighter-rouge">Is</code> state, or a <code class="language-plaintext highlighter-rouge">Ds</code> state.</p> <p>Apparently, <code class="language-plaintext highlighter-rouge">D</code> processes are waiting for I/O in a non-interruptible state. <code class="language-plaintext highlighter-rouge">I</code> processes are idle, and are in an interruptible state. If I tried to poke around in <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;</code> too much, even procfs would freeze.</p> <h2 id="wchan">wchan</h2> <p>The first useful thing I learned was the <code class="language-plaintext highlighter-rouge">wchan</code> column in <code class="language-plaintext highlighter-rouge">ps</code>. <code class="language-plaintext highlighter-rouge">wchan</code> means wait channel - it tells you what stream (or whatever the definition of a channel is) is a process waiting on.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ -eo ppid,pid,user,stat,pcpu,comm,wchan -p 21061 | grep stoch
  21028   21060 ankushj  Zs    0.3 stoch &lt;defunct&gt; -
  21028   21061 ankushj  Is    0.4 stochastic_subg ptlrpc_set_wait
  21028   21062 ankushj  Zs    0.2 stoch &lt;defunct&gt; -
  21028   21068 ankushj  Zs    0.2 stoch &lt;defunct&gt; -
  21028   21069 ankushj  Ds    0.4 stochastic_subg rwsem_down_write_slowpath
  21028   21070 ankushj  Ds    0.4 stochastic_subg rwsem_down_write_slowpath
  21028   21071 ankushj  Zs    0.4 stoch &lt;defunct&gt; -
  21028   21072 ankushj  Zs    0.4 stoch &lt;defunct&gt; -
  ...
</code></pre></div></div> <p>Bingo - we have some function names to blame. Apparently we can get more context around these without needing to attach a debugger, which we can not.</p> <p>We can <code class="language-plaintext highlighter-rouge">cat</code> <code class="language-plaintext highlighter-rouge">/proc/&lt;pid&gt;/stack</code> to get a stack trace from a process without a debugger. Thankfully this works.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo cat /proc/21061/stack

[&lt;0&gt;] ptlrpc_set_wait+0x5e8/0x730 [ptlrpc]
[&lt;0&gt;] ptlrpc_queue_wait+0x88/0x230 [ptlrpc]
[&lt;0&gt;] ldlm_cli_enqueue+0x436/0x990 [ptlrpc]
[&lt;0&gt;] mdc_enqueue_base+0x2f2/0x1c90 [mdc]
[&lt;0&gt;] mdc_intent_lock+0x212/0x530 [mdc]
[&lt;0&gt;] lmv_intent_lock+0x385/0x16b0 [lmv]
[&lt;0&gt;] ll_lookup_it+0x7ad/0x20e0 [lustre]
[&lt;0&gt;] ll_atomic_open+0x198/0xff0 [lustre]
[&lt;0&gt;] lookup_open+0x364/0x6e0
[&lt;0&gt;] do_last+0x2cb/0x900
[&lt;0&gt;] path_openat+0x8d/0x290
[&lt;0&gt;] do_filp_open+0x91/0x100
[&lt;0&gt;] do_sys_open+0x17e/0x290
[&lt;0&gt;] __x64_sys_openat+0x20/0x30
[&lt;0&gt;] do_syscall_64+0x57/0x190
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo cat /proc/21065/stack

[&lt;0&gt;] rwsem_down_write_slowpath+0x244/0x4d0
[&lt;0&gt;] do_last+0x2b1/0x900
[&lt;0&gt;] path_openat+0x8d/0x290
[&lt;0&gt;] do_filp_open+0x91/0x100
[&lt;0&gt;] do_sys_open+0x17e/0x290
[&lt;0&gt;] __x64_sys_openat+0x20/0x30
[&lt;0&gt;] do_syscall_64+0x57/0x190
[&lt;0&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9
</code></pre></div></div> <p>This is beautiful! Instead of gaslighting ourselves and slipping into an existential crisis, we can point our fingers elsewhere.</p> <h2 id="conclusion">Conclusion</h2> <p>Both these stack traces indicate getting stuck on a File I/O call. The first trace is more specific - it tells us that it is a Lustre I/O. Lustre is a parallel filesystem.</p> <p>Turns out that one of my Lustre OSTs was dead. That would explain this behavior - the Lustre module sits in the kernel, and is blocked trying to reach that OST. Maybe there are good reasons a timeout there can not be handled gracefully, maybe there are not. But a kernel module freezing is tough - a whole bunch of debugging options go for a toss. Thankfully some still work!</p>]]></content><author><name></name></author><category term="debugging"/><category term="#systems"/><category term="#gdb"/><summary type="html"><![CDATA[The problem here was being caused by a kernel module misbehaving because a remote service was misbehaving.]]></summary></entry><entry><title type="html">Replication Part 1 - Primary-Backup vs Chain Replication</title><link href="https://ankushja.in/blog/2024/replication-part-1/" rel="alternate" type="text/html" title="Replication Part 1 - Primary-Backup vs Chain Replication"/><published>2024-04-11T17:35:30+00:00</published><updated>2024-04-11T17:35:30+00:00</updated><id>https://ankushja.in/blog/2024/replication-part-1</id><content type="html" xml:base="https://ankushja.in/blog/2024/replication-part-1/"><![CDATA[<p>The purpose with this series of posts, to the extent it is followed through, is to arrive at different replication schemes by applying diffs to some well-understood schemes. This exercise should help us understand the tradeoffs, the design space, and reason about whether a point in the design space is pareto-optimal, that is, can its pros be realized by a diff over some other design point with less cons.</p> <p>This is somewhat facetious, but IMO there are two classes of people in the distributed systems world — Leslie Lamport, who stared into the soul of replication and figured out something fundamental a couple of decades ago and has since been trying to tell us all that everything in replication is a specialization of that [principle][1,2,3], and everyone else.</p> <p>Part 1 will cover 3 basic designs. For now, we will assume single-key operations and key-value semantics, and see how far this simple model gets us.</p> <p>Let me also try to assign relative perf numbers for these, where 100 is the max perf for a single-node design (writes and reads are assumed to be symmetric cost-wise, so a 100 perf for writes and reads just means that any combination of the two will have a perf of 100. Perf = latency = throughput as we can assume one request in flight at a time, WLOG?). So a perf of 500 implies 5X the read throughput of a single node.</p> <h5 id="1-a-single-worker">1. A Single Worker</h5> <p>(Worker could be a core, or a node, or a datacenter even. We relax consistency for performance all the way down.)</p> <p>Design: a single worker, receives all requests, applies them sequentially, writes see reads, works great, no issues.</p> <p>Consistency: Strict Failure: No tolerance Read Perf: 100 Write Perf: 100</p> <h5 id="2-primary-backup">2. Primary-Backup</h5> <p>Two workers. All requests are handled by the primary. All writes are synchronously copied to backup. Backup kicks in on failure.</p> <p>Consistency: Strict Failure: Tolerates 1 Read Perf: 100 Write Perf: 50 (assuming synchronous copying = as costly as query)</p> <h5 id="2a-primary-backup-with-async-replication">2a. Primary-Backup with Async Replication</h5> <p>Primary can ACK to a request before replication to backup is completed. It is a meaningless design point because async replication has no tolerance for failures. The delta between the two can be lost.</p> <h5 id="2b-primary-backup-with-backup-serving-reads">2b. Primary-Backup with Backup Serving Reads</h5> <p>The backup is right there, and it is idle. Can we use it?</p> <p>We won’t use it for writes, because concurrent writes to the same key can cause conflicts and we do not yet know how to resolve them. Also we have not evolved and coordination mechanism yet. But we could use it for reads… right?</p> <p>But it should respond to reads without requiring coordination with the primary, otherwise we have not really gained anything in terms of performance. But that has consistency implications — what if the primary is concurrently updating the value we returned?</p> <p>This works under two cases:</p> <ol> <li>A looser consistency model, Sequential Consistency, is acceptable, OR</li> <li>The Primary and Backup can establish a coordination mechanism that is more efficient than what the clients can do (sharding keys is one example of such a mechanism)</li> </ol> <p>For case 2b1.</p> <p>Consistency: Sequential Failure: Tolerates 1 Read Perf: 200 Write Perf: 50</p> <h5 id="3-chain-replication">3. Chain Replication</h5> <p>Chain Replication is just a generalization of 2b. Instead of having one replica, you can have N replicas. In fact, for 1:N primary-read serving backup systems, I think these two are logically equivalent:</p> <ol> <li>The primary chaining writes sequentially through replicas</li> <li>The primary broadcasting writes to all replicas</li> </ol> <p>First option increases latency by N, and the second option requires the primary to do $N\times$ more communication.</p> <p>I think the two only have perf implications, but are logically equivalent to each other and to 2b.</p> <h4 id="references">References</h4>]]></content><author><name></name></author><category term="systems"/><summary type="html"><![CDATA[Simplifying scheme soup with successive steps]]></summary></entry></feed>